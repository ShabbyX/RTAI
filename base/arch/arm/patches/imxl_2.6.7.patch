diff --git a/Makefile b/Makefile
index 43434a7..e0a88f4 100644
--- a/Makefile
+++ b/Makefile
@@ -486,7 +486,7 @@ export MODLIB

 ifeq ($(KBUILD_EXTMOD),)
 core-y		+= kernel/ mm/ fs/ ipc/ security/ crypto/
-
+core-$(CONFIG_ADEOS) += adeos/
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
 		     $(net-y) $(net-m) $(libs-y) $(libs-m)))
diff --git a/adeos/Kconfig b/adeos/Kconfig
new file mode 100644
index 0000000..023ea23
--- /dev/null
+++ b/adeos/Kconfig
@@ -0,0 +1,24 @@
+menu "Adeos support"
+
+config ADEOS
+	tristate "Adeos support"
+	default y
+	---help---
+	  Activate this option if you want the Adeos nanokernel to be
+	  compiled in.
+	  Adeos Implementation is based on rtai-3.2 and adeos-patch for pxa255
+	  contained in rtai-3.2 sources.
+	  The patch also depends on linux-2.6.7-imx4.diff (pengutronix) and
+	  parts of pxa255_2.6.7-bk6-karo.patch.
+
+config ADEOS_CORE
+	bool
+	depends on ADEOS
+	default y
+
+config ADEOS_THREADS
+	def_bool ADEOS_CORE
+
+endmenu
+
+
diff --git a/adeos/Makefile b/adeos/Makefile
new file mode 100644
index 0000000..29219a7
--- /dev/null
+++ b/adeos/Makefile
@@ -0,0 +1,9 @@
+#
+# Makefile for the Adeos layer.
+#
+
+obj-$(CONFIG_ADEOS)	+= adeos.o
+
+adeos-objs		:= generic.o
+
+adeos-$(CONFIG_ARM)	+= armv.o
diff --git a/adeos/armv.c b/adeos/armv.c
new file mode 100644
index 0000000..0a0bad4
--- /dev/null
+++ b/adeos/armv.c
@@ -0,0 +1,477 @@
+/*
+ *   linux/adeos/arm.c
+ *
+ *   Copyright (C) 2003,2004 Philippe Gerum.
+ *   RTAI/ARM over Adeos rewrite for PXA255_2.6.7:
+ *   Copyright (c) 2005 Stefano Gafforelli (stefano.gafforelli@tiscali.it)
+ *   Copyright (c) 2005 Luca Pizzi (lucapizzi@hotmail.com)
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-dependent ADEOS support for ARM.
+ */
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <asm/hardware.h>
+#include <asm/mach/irq.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+#include <asm/io.h>
+#include <asm-arm/errno.h>
+#include <asm-arm/smp.h>
+
+extern spinlock_t __adeos_pipelock;
+
+extern struct list_head __adeos_pipeline;
+
+extern struct pt_regs *regs;
+
+extern struct irqdesc *desc;
+
+extern struct irqdesc irq_desc[];
+
+extern struct pt_regs __adeos_irq_regs;
+
+asmlinkage void asm_do_IRQ(int irq, struct pt_regs *regs);
+
+
+extern struct irqchip __adeos_std_irq_chip[NR_IRQS];
+
+static void __adeos_override_irq_mask (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+
+    __adeos_std_irq_chip[irq].mask(irq);
+    __adeos_lock_irq(adp_cpu_current[cpuid],cpuid,irq);
+
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+static void __adeos_override_irq_unmask (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+
+    __adeos_unlock_irq(adp_cpu_current[cpuid],irq);
+    __adeos_std_irq_chip[irq].unmask(irq);
+
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+static void __adeos_override_irq_mask_ack (unsigned irq)
+
+{
+    unsigned long adflags, hwflags;
+    adeos_declare_cpuid;
+    adeos_hw_local_irq_save(hwflags);
+    adflags = adeos_test_and_stall_pipeline();
+
+#ifdef CONFIG_PREEMPT
+    preempt_disable();
+#endif /* CONFIG_PREEMPT */
+
+    __adeos_std_irq_chip[irq].ack(irq);
+    /* No locking here, since this would prevent to sync the stage. */
+
+#ifdef CONFIG_PREEMPT
+    preempt_enable_no_resched();
+#endif /* CONFIG_PREEMPT */
+
+    adeos_restore_pipeline_nosync(adp_cpu_current[cpuid],adflags,cpuid);
+    adeos_hw_local_irq_restore(hwflags);
+}
+
+static int __adeos_ack_irq (unsigned irq)
+
+{
+      __adeos_std_irq_chip[irq].ack(irq);
+      return 1;
+}
+
+/* __adeos_enable_pipeline() -- Take over the interrupt control from
+   the root domain (i.e. Linux). After this routine has returned, all
+   interrupts go through the pipeline. */
+
+void __adeos_enable_pipeline (void)
+
+{
+    unsigned long flags;
+    unsigned irq;
+    flags = adeos_critical_enter(NULL);
+
+    /* First, virtualize all interrupts from the root domain. */
+
+    for (irq = 0; irq < NR_IRQS; irq++)
+	adeos_virtualize_irq(irq,
+			   (void (*)(unsigned))&asm_do_IRQ,
+			     __adeos_ack_irq,
+			     IPIPE_HANDLE_MASK|IPIPE_PASS_MASK);
+
+    /* Interpose on the IRQ control routines so we can make them
+       atomic using hw masking and prevent the interrupt log from
+       being untimely flushed. */
+
+    for (irq = 0; irq < NR_IRQS; irq++)
+	__adeos_std_irq_chip[irq] = *irq_desc[irq].chip;
+
+    /* The original controller structs are often shared, so we first
+       save them all before changing any of them. Notice that we don't
+       redirect the ack handler since the relevant IC management code
+       is already Adeos-aware. */
+
+    for (irq = 0; irq < NR_IRQS; irq++)
+	{
+	if (irq_desc[irq].chip->mask != NULL) {
+	    irq_desc[irq].chip->mask = __adeos_override_irq_mask;
+	}
+	if (irq_desc[irq].chip->unmask != NULL) {
+	    irq_desc[irq].chip->unmask = __adeos_override_irq_unmask;
+	}
+	if (irq_desc[irq].chip->ack != NULL) {
+	    irq_desc[irq].chip->ack = __adeos_override_irq_mask_ack;
+	}
+    }
+
+#ifdef CONFIG_ADEOS_MODULE
+    adp_pipelined = 1;
+#endif /* CONFIG_ADEOS_MODULE */
+
+    adeos_critical_exit(flags);
+
+}
+
+/* __adeos_disable_pipeline() -- Disengage the pipeline. */
+
+void __adeos_disable_pipeline (void)
+
+{
+    unsigned long flags;
+    unsigned irq;
+
+    flags = adeos_critical_enter(NULL);
+
+    /* Restore interrupt controllers. */
+
+    for (irq = 0; irq < NR_IRQS; irq++)
+	*irq_desc[irq].chip = __adeos_std_irq_chip[irq];
+
+#ifdef CONFIG_ADEOS_MODULE
+    adp_pipelined = 0;
+#endif /* CONFIG_ADEOS_MODULE */
+
+    adeos_critical_exit(flags);
+}
+
+/* adeos_virtualize_irq_from() -- Attach a handler (and optionally a
+   hw acknowledge routine) to an interrupt for the given domain. */
+
+int adeos_virtualize_irq_from (adomain_t *adp,
+			       unsigned irq,
+			       void (*handler)(unsigned irq),
+			       int (*acknowledge)(unsigned irq),
+			       unsigned modemask)
+{
+    unsigned long flags;
+    int err;
+
+    if (irq >= IPIPE_NR_IRQS)
+	return -EINVAL;
+
+    if (adp->irqs[irq].control & IPIPE_SYSTEM_MASK)
+	return -EPERM;
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (handler != NULL)
+	{
+	/* A bit of hack here: if we are re-virtualizing an IRQ just
+	   to change the acknowledge routine by passing the special
+	   ADEOS_SAME_HANDLER value, then allow to recycle the current
+	   handler for the IRQ. This allows Linux device drivers
+	   managing shared IRQ lines to call adeos_virtualize_irq() in
+	   addition to request_irq() just for the purpose of
+	   interposing their own shared acknowledge routine. */
+
+	if (handler == ADEOS_SAME_HANDLER)
+	    {
+	    handler = adp->irqs[irq].handler;
+
+	    if (handler == NULL)
+		{
+		err = -EINVAL;
+		goto unlock_and_exit;
+		}
+	    }
+	else if ((modemask & IPIPE_EXCLUSIVE_MASK) != 0 &&
+		 adp->irqs[irq].handler != NULL)
+	    {
+	    err = -EBUSY;
+	    goto unlock_and_exit;
+	    }
+
+	if ((modemask & (IPIPE_SHARED_MASK|IPIPE_PASS_MASK)) == IPIPE_SHARED_MASK)
+	    {
+	    err = -EINVAL;
+	    goto unlock_and_exit;
+	    }
+
+	if ((modemask & IPIPE_STICKY_MASK) != 0)
+	    modemask |= IPIPE_HANDLE_MASK;
+	}
+    else
+	modemask &= ~(IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK|IPIPE_SHARED_MASK);
+
+    if (acknowledge == NULL)
+	{
+	if ((modemask & IPIPE_SHARED_MASK) == 0)
+	    /* Acknowledge handler unspecified -- this is ok in
+	       non-shared management mode, but we will force the use
+	       of the Linux-defined handler instead. */
+	    acknowledge = adp_root->irqs[irq].acknowledge;
+	else
+	    {
+	    /* A valid acknowledge handler to be called in shared mode
+	       is required when declaring a shared IRQ. */
+	    err = -EINVAL;
+	    goto unlock_and_exit;
+	    }
+	}
+
+    adp->irqs[irq].handler = handler;
+    adp->irqs[irq].acknowledge = acknowledge;
+    adp->irqs[irq].control = modemask;
+
+    if (irq < NR_IRQS &&
+	handler != NULL &&
+	!adeos_virtual_irq_p(irq) &&
+	(modemask & IPIPE_ENABLE_MASK) != 0)
+	{
+	if (adp != adp_current)
+	    {
+	    /* IRQ enable/disable state is domain-sensitive, so we may
+	       not change it for another domain. What is allowed
+	       however is forcing some domain to handle an interrupt
+	       source, by passing the proper 'adp' descriptor which
+	       thus may be different from adp_current. */
+	    err = -EPERM;
+	    goto unlock_and_exit;
+	    }
+
+	enable_irq(irq);
+	}
+
+    err = 0;
+
+unlock_and_exit:
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return err;
+}
+
+/* adeos_control_irq() -- Change an interrupt mode. This affects the
+   way a given interrupt is handled by ADEOS for the current
+   domain. setmask is a bitmask telling whether:
+   - the interrupt should be passed to the domain (IPIPE_HANDLE_MASK),
+     and/or
+   - the interrupt should be passed down to the lower priority domain(s)
+     in the pipeline (IPIPE_PASS_MASK).
+   This leads to four possibilities:
+   - PASS only => Ignore the interrupt
+   - HANDLE only => Terminate the interrupt (process but don't pass down)
+   - PASS + HANDLE => Accept the interrupt (process and pass down)
+   - <none> => Discard the interrupt
+   - DYNAMIC is currently an alias of HANDLE since it marks an interrupt
+   which is processed by the current domain but not implicitely passed
+   down to the pipeline, letting the domain's handler choose on a case-
+   by-case basis whether the interrupt propagation should be forced
+   using adeos_propagate_irq().
+   clrmask clears the corresponding bits from the control field before
+   setmask is applied.
+*/
+
+int adeos_control_irq (unsigned irq,
+		       unsigned clrmask,
+		       unsigned setmask)
+{
+    struct irqdesc *desc;
+    unsigned long flags;
+    if (irq >= IPIPE_NR_IRQS)
+	return -EINVAL;
+
+    if (adp_current->irqs[irq].control & IPIPE_SYSTEM_MASK)
+	return -EPERM;
+
+    if (((setmask|clrmask) & IPIPE_SHARED_MASK) != 0)
+	return -EINVAL;
+
+    desc = irq_desc + irq;
+
+    if (adp_current->irqs[irq].handler == NULL)
+	setmask &= ~(IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK);
+
+    if ((setmask & IPIPE_STICKY_MASK) != 0)
+	setmask |= IPIPE_HANDLE_MASK;
+
+    if ((clrmask & (IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK)) != 0)	/* If one goes, both go. */
+	clrmask |= (IPIPE_HANDLE_MASK|IPIPE_STICKY_MASK);
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    adp_current->irqs[irq].control &= ~clrmask;
+    adp_current->irqs[irq].control |= setmask;
+
+    if ((setmask & IPIPE_ENABLE_MASK) != 0)
+	enable_irq(irq);
+    else if ((clrmask & IPIPE_ENABLE_MASK) != 0)
+	disable_irq(irq);
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return 0;
+}
+
+static inline unsigned long __current_domain_access_control (void)
+
+{
+    unsigned long domain_access_control;
+    __asm__ __volatile__ ("mrc p15, 0, %0, c3, c0" : "=r" (domain_access_control));
+    return domain_access_control;
+}
+
+void __adeos_init_domain (adomain_t *adp, adattr_t *attr)
+
+{
+    int estacksz = attr->estacksz > 0 ? attr->estacksz : 8192, _cpuid;
+    unsigned long init_arch_flags, init_domain_access_control;
+    adeos_declare_cpuid;
+    adeos_load_cpuid();
+    adeos_hw_local_irq_flags(init_arch_flags);
+    init_domain_access_control = __current_domain_access_control();
+
+    for (_cpuid = 0; _cpuid < smp_num_cpus; _cpuid++)
+	{
+	int **psp = &adp->esp[_cpuid];
+
+	adp->estackbase[_cpuid] = (int *)kmalloc(estacksz,GFP_KERNEL);
+
+	if (adp->estackbase[_cpuid] == NULL)
+	    panic("Adeos: No memory for domain stack on CPU #%d",_cpuid);
+
+	adp->esp[_cpuid] = adp->estackbase[_cpuid];
+	**psp = 0;
+	*psp = (int *)(((unsigned long)*psp + estacksz - 60) & ~0x3);
+	*--(*psp) = (int)attr->entry; /* r14=lr */
+	*--(*psp) = 0;		/* r11=fp */
+	*--(*psp) = 0;		/* r10=sl */
+	*--(*psp) = 0;		/* r9 */
+	*--(*psp) = 0;		/* r8 */
+	*--(*psp) = 0;		/* r7 */
+	*--(*psp) = 0;		/* r6 */
+	*--(*psp) = 0;		/* r5 */
+	*--(*psp) = 0;		/* r4 */
+	*--(*psp) = 0;		/* r3 */
+	*--(*psp) = 0;		/* r2 */
+	*--(*psp) = 0;		/* r1 */
+	*--(*psp) = (_cpuid == cpuid); /* r0=iflag */
+	*--(*psp) = init_arch_flags;	/* cpsr_SVC */
+	*--(*psp) = init_domain_access_control;
+	}
+}
+
+void __adeos_cleanup_domain (adomain_t *adp)
+
+{
+    adeos_unstall_pipeline_from(adp);
+
+    if (adp->estackbase[0] != NULL)
+	kfree(adp->estackbase[0]);
+}
+
+int adeos_get_sysinfo (adsysinfo_t *info)
+
+{
+    info->ncpus = 1;
+    info->cpufreq = CLOCK_TICK_RATE;
+    info->archdep.tmirq = ADEOS_TIMER_IRQ; // ADEOS_TIMER_IRQ TIM1_INT
+
+    return 0;
+}
+
+int adeos_tune_timer (unsigned long ns, int flags)
+
+{
+    unsigned long x, hz;
+
+    if (flags & ADEOS_RESET_TIMER){
+	hz = HZ;printk("hz value %ld \n",hz);}
+    else
+	{
+	if (ns < 50000)	/* FIXME: this needs to be defined more accurately */
+	    return -EINVAL;
+
+	hz = 1000000000 / ns;
+	printk("hz value %ld \n",hz);
+	}
+
+    adeos_hw_local_irq_save(x);
+
+    __adeos_tune_timer(hz);
+
+    adeos_hw_local_irq_restore(x);
+
+    return 0;
+}
+
+/* adeos_trigger_ipi() -- Send the ADEOS service IPI to other
+   processors. Not implemented for this architecture. */
+
+int adeos_trigger_ipi (int _cpuid)
+
+{
+    printk(KERN_WARNING "Adeos: Call to unimplemented adeos_trigger_ipi() from %s\n",adp_current->name);
+    return 0;
+}
+
diff --git a/adeos/generic.c b/adeos/generic.c
new file mode 100644
index 0000000..2ca2b54
--- /dev/null
+++ b/adeos/generic.c
@@ -0,0 +1,794 @@
+/*
+ *   linux/adeos/generic.c
+ *
+ *   Copyright (C) 2002 Philippe Gerum.
+ *   RTAI/ARM over Adeos rewrite for PXA255_2.6.7:
+ *   Copyright (c) 2005 Stefano Gafforelli (stefano.gafforelli@tiscali.it)
+ *   Copyright (c) 2005 Luca Pizzi (lucapizzi@hotmail.com)
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-independent ADEOS services.
+ */
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/adeos.h>
+#include <linux/irq.h>
+
+MODULE_DESCRIPTION("Adeos nanokernel");
+MODULE_AUTHOR("Philippe Gerum");
+MODULE_LICENSE("GPL");
+
+extern spinlock_t __adeos_pipelock;
+
+extern struct list_head __adeos_pipeline;
+
+/* adeos_register_domain() -- Add a new domain to the system. All
+   client domains must call this routine to register themselves to
+   ADEOS before using its services. */
+
+int adeos_register_domain (adomain_t *adp, adattr_t *attr)
+
+{
+    struct list_head *pos;
+    unsigned long flags;
+    int n;
+    if (adp_current != adp_root)
+	{
+	printk(KERN_WARNING "Adeos: Only the root domain may register a new domain.\n");
+	return -EPERM;
+	}
+
+    flags = adeos_critical_enter(NULL);
+
+    list_for_each(pos,&__adeos_pipeline) {
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+	if (_adp->domid == attr->domid)
+            break;
+    }
+
+    adeos_critical_exit(flags);
+
+    if (pos != &__adeos_pipeline)
+	/* A domain with the given id already exists -- fail. */
+	return -EBUSY;
+
+    for (n = 0; n < ADEOS_NR_CPUS; n++)
+	adp->cpudata[n].status = 0;
+
+    /* A special case for domains who won't process events (i.e. no
+       entry). We have to mark them as suspended so that
+       adeos_suspend_domain() won't consider them, unless they
+       _actually_ receive events, which would lead to a panic
+       situation since they have no stack context... :o> */
+
+    for (n = 0; n < ADEOS_NR_CPUS; n++)
+	{
+	set_bit(IPIPE_SLEEP_FLAG,&adp->cpudata[n].status);
+
+	if (attr->entry == NULL)
+	    adp->estackbase[n] = NULL;
+	}
+
+    adp->name = attr->name;
+    adp->priority = attr->priority;
+    adp->domid = attr->domid;
+    adp->dswitch = attr->dswitch;
+    adp->flags = 0;
+    adp->ptd_setfun = attr->ptdset;
+    adp->ptd_getfun = attr->ptdget;
+    adp->ptd_keymap = 0;
+    adp->ptd_keycount = 0;
+    adp->ptd_keymax = attr->nptdkeys;
+
+    for (n = 0; n < ADEOS_NR_EVENTS; n++)
+	/* Event handlers must be cleared before the i-pipe stage is
+	   inserted since an exception may occur on behalf of the new
+	   emerging domain. */
+	adp->events[n].handler = NULL;
+
+    if (attr->entry != NULL)
+	__adeos_init_domain(adp,attr);
+
+    /* Insert the domain in the interrupt pipeline last, so it won't
+       be resumed for processing interrupts until it has a valid stack
+       context. */
+
+    __adeos_init_stage(adp);
+
+    INIT_LIST_HEAD(&adp->p_link);
+
+    flags = adeos_critical_enter(NULL);
+
+    list_for_each(pos,&__adeos_pipeline) {
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+	if (adp->priority > _adp->priority)
+            break;
+    }
+
+    list_add_tail(&adp->p_link,pos);
+
+    adeos_critical_exit(flags);
+
+    printk(KERN_WARNING "Adeos: Domain %s registered.\n",adp->name);
+
+    /* Finally, allow the new domain to perform its initialization
+       chores on behalf of its own stack context. */
+
+    if (attr->entry != NULL)
+	{
+	adeos_declare_cpuid;
+
+	adeos_get_cpu(flags);
+
+	__adeos_switch_to(adp,cpuid);
+
+	adeos_load_cpuid();	/* Processor might have changed. */
+
+	if (!test_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status) &&
+	    adp_root->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+
+	adeos_put_cpu(flags);
+	}
+
+    return 0;
+}
+
+/* adeos_unregister_domain() -- Remove a domain from the system. All
+   client domains must call this routine to unregister themselves from
+   the ADEOS layer. */
+
+int adeos_unregister_domain (adomain_t *adp)
+
+{
+    unsigned long flags;
+    unsigned event;
+
+    if (adp_current != adp_root)
+	{
+	printk(KERN_WARNING "Adeos: Only the root domain may unregister a domain.\n");
+	return -EPERM;
+	}
+
+    if (adp == adp_root)
+	{
+	printk(KERN_WARNING "Adeos: Cannot unregister the root domain.\n");
+	return -EPERM;
+	}
+
+    for (event = 0; event < ADEOS_NR_EVENTS; event++)
+	/* Need this to update the monitor count. */
+	adeos_catch_event(event,NULL);
+
+#ifdef CONFIG_SMP
+    {
+    unsigned irq;
+    int _cpuid;
+
+    /* In the SMP case, wait for the logged events to drain on other
+       processors before eventually removing the domain from the
+       pipeline. */
+
+    adeos_unstall_pipeline_from(adp);
+
+    flags = adeos_critical_enter(NULL);
+
+    for (irq = 0; irq < IPIPE_NR_IRQS; irq++)
+	{
+	clear_bit(IPIPE_HANDLE_FLAG,&adp->irqs[irq].control);
+	clear_bit(IPIPE_STICKY_FLAG,&adp->irqs[irq].control);
+	set_bit(IPIPE_PASS_FLAG,&adp->irqs[irq].control);
+	}
+
+    adeos_critical_exit(flags);
+
+    for (_cpuid = 0; _cpuid < smp_num_cpus; _cpuid++)
+	{
+	for (irq = 0; irq < IPIPE_NR_IRQS; irq++)
+	    while (adp->cpudata[_cpuid].irq_hits[irq] > 0)
+		cpu_relax();
+
+	while (test_bit(IPIPE_XPEND_FLAG,&adp->cpudata[_cpuid].status))
+	    cpu_relax();
+
+	while (!test_bit(IPIPE_SLEEP_FLAG,&adp->cpudata[_cpuid].status))
+	     cpu_relax();
+	}
+    }
+#endif /* CONFIG_SMP */
+
+    /* Simply remove the domain from the pipeline and we are almost
+       done. */
+
+    flags = adeos_critical_enter(NULL);
+    list_del_init(&adp->p_link);
+    adeos_critical_exit(flags);
+
+    __adeos_cleanup_domain(adp);
+
+    printk(KERN_WARNING "Adeos: Domain %s unregistered.\n",adp->name);
+
+    return 0;
+}
+
+/* adeos_renice_domain() -- Change the priority of the current
+   domain. This affects the position of the domain in the interrupt
+   pipeline. The greater the priority value, the earlier the domain is
+   informed of incoming events when the pipeline is processed. This
+   routine causes a round-robin effect if newpri == oldpri. */
+
+void adeos_renice_domain (int newpri)
+
+{
+    adomain_t *adp, *nadp = NULL;
+    unsigned long lflags, xflags;
+    struct list_head *pos;
+    adeos_declare_cpuid;
+
+    /* We do want adeos_critical_exit() to leave the IRQs masked, so
+       we first clear the interrupt bit before calling
+       adeos_critical_enter(). */
+
+    adeos_lock_cpu(lflags);
+
+    adp = adp_cpu_current[cpuid];
+
+    xflags = adeos_critical_enter(NULL);
+
+    list_del_init(&adp->p_link);
+
+    list_for_each(pos,&__adeos_pipeline) {
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	if (newpri > _adp->priority)
+            break;
+
+	/* While scanning the pipeline from its head to the current
+	   domain's new position, pick the first domain which is
+	   entitled to preempt us. Such domain must be either:
+	   o in a preempted state (i.e. !sleeping),
+	   o or sleeping and unstalled with events to process. */
+
+	if (nadp == NULL &&
+	    (!test_bit(IPIPE_SLEEP_FLAG,&_adp->cpudata[cpuid].status) ||
+	     (!test_bit(IPIPE_STALL_FLAG,&_adp->cpudata[cpuid].status) &&
+	      _adp->cpudata[cpuid].irq_pending_hi != 0) ||
+	     test_bit(IPIPE_XPEND_FLAG,&_adp->cpudata[cpuid].status)))
+	    nadp = _adp;
+    }
+
+    list_add_tail(&adp->p_link,pos);
+    adp->priority = newpri;
+
+    /* On SMP systems, we release the other CPUs but we still keep the
+       local IRQs masked so that we can't jump to a stale domain. */
+
+    adeos_critical_exit(xflags);
+
+    if (nadp == NULL)
+	goto release_cpu_and_exit;
+
+    __adeos_switch_to(nadp,cpuid);
+
+    adeos_load_cpuid(); /* Processor might have changed. */
+
+    /* Try sync'ing pending interrupts on return from our preemption
+       point. */
+
+    if (!test_bit(IPIPE_STALL_FLAG,&adp_cpu_current[cpuid]->cpudata[cpuid].status) &&
+	adp_cpu_current[cpuid]->cpudata[cpuid].irq_pending_hi != 0)
+	__adeos_sync_stage();
+
+    /* Note that we only need to sync interrupts here, since other
+       kind of events (i.e. synchronous ones) cannot flow across the
+       domain which triggers them down the pipeline. Since a more
+       prioritary domain was running up to now, there is no chance for
+       us to have such event pending. */
+
+release_cpu_and_exit:
+
+    adeos_unlock_cpu(lflags);
+}
+
+int __adeos_schedule_irq (unsigned irq, struct list_head *head)
+
+{
+    struct list_head *ln;
+    unsigned long flags;
+    adeos_declare_cpuid;
+
+    if (irq >= IPIPE_NR_IRQS ||
+	(adeos_virtual_irq_p(irq) && !test_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map)))
+	return -EINVAL;
+
+    adeos_lock_cpu(flags);
+
+    ln = head;
+
+    while (ln != &__adeos_pipeline)
+	{
+	adomain_t *adp = list_entry(ln,adomain_t,p_link);
+
+	if (test_bit(IPIPE_HANDLE_FLAG,&adp->irqs[irq].control))
+	    {
+	    adp->cpudata[cpuid].irq_hits[irq]++;
+	    __adeos_set_irq_bit(adp,cpuid,irq);
+	    adeos_unlock_cpu(flags);
+	    return 1;
+	    }
+
+	ln = adp->p_link.next;
+	}
+
+    adeos_unlock_cpu(flags);
+
+    return 0;
+}
+
+/* adeos_propagate_irq() -- Force a given IRQ propagation on behalf of
+   a running interrupt handler to the next domain down the pipeline.
+   Returns non-zero if a domain has received the interrupt
+   notification, zero otherwise.
+   This call is useful for handling shared interrupts among domains.
+   e.g. pipeline = [domain-A]---[domain-B]...
+   Both domains share IRQ #X.
+   - domain-A handles IRQ #X but does not pass it down (i.e. Terminate
+   or Dynamic interrupt control mode)
+   - domain-B handles IRQ #X (i.e. Terminate or Accept interrupt
+   control modes).
+   When IRQ #X is raised, domain-A's handler determines whether it
+   should process the interrupt by identifying its source. If not,
+   adeos_propagate_irq() is called so that the next domain down the
+   pipeline which handles IRQ #X is given a chance to process it. This
+   process can be repeated until the end of the pipeline is
+   reached. */
+int adeos_propagate_irq (unsigned irq) {
+
+   return __adeos_schedule_irq(irq,adp_current->p_link.next);
+}
+
+/* adeos_schedule_irq() -- Almost the same as adeos_propagate_irq(),
+   but attempts to pend the interrupt for the current domain first. */
+
+int adeos_schedule_irq (unsigned irq) {
+
+    return __adeos_schedule_irq(irq,&adp_current->p_link);
+}
+
+unsigned long adeos_set_irq_affinity (unsigned irq, unsigned long cpumask)
+
+{
+#ifdef CONFIG_SMP
+     if (irq >= IPIPE_NR_XIRQS)
+	 /* Allow changing affinity of external IRQs only. */
+	 return 0;
+
+     if (smp_num_cpus > 1)
+	 /* Allow changing affinity of external IRQs only. */
+	 return __adeos_set_irq_affinity(irq,cpumask);
+#endif /* CONFIG_SMP */
+
+    return 0;
+}
+
+/* adeos_free_irq() -- Return a previously allocated virtual/soft
+   pipelined interrupt to the pool of allocatable interrupts. */
+
+int adeos_free_irq (unsigned irq)
+
+{
+    if (irq >= IPIPE_NR_IRQS)
+	return -EINVAL;
+
+    clear_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map);
+
+    return 0;
+}
+
+/* adeos_unstall_pipeline_from() -- Unstall the interrupt pipeline and
+   synchronize pending events from a given domain. */
+
+void adeos_unstall_pipeline_from (adomain_t *adp)
+
+{
+    struct list_head *pos;
+    unsigned long flags;
+    adeos_declare_cpuid;
+
+    adeos_get_cpu(flags);
+
+    __clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+
+    if (adp == adp_cpu_current[cpuid])
+	{
+	if (adp->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+
+	goto release_cpu_and_exit;
+	}
+
+    /* Attempt to flush all events that might be pending at the
+       unstalled domain level. This code is roughly lifted from
+       drivers/adeos/x86.c:__adeos_walk_pipeline(). */
+
+    list_for_each(pos,&__adeos_pipeline) {
+
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	if (test_bit(IPIPE_STALL_FLAG,&_adp->cpudata[cpuid].status))
+	    break; /* Stalled stage -- do not go further. */
+
+	if (_adp->cpudata[cpuid].irq_pending_hi != 0)
+	    {
+	    /* Since the critical IPI might be triggered by the
+	       following actions, the current domain might not be
+	       linked to the pipeline anymore after its handler
+	       returns on SMP boxen, even if the domain remains valid
+	       (see adeos_unregister_domain()), so don't make any
+	       hazardous assumptions here. */
+
+	    if (_adp == adp_cpu_current[cpuid])
+
+        	__adeos_sync_stage();
+	    else
+		{
+	__adeos_switch_to(_adp,cpuid);
+
+		adeos_load_cpuid(); /* Processor might have changed. */
+
+		if (!test_bit(IPIPE_STALL_FLAG,&adp_cpu_current[cpuid]->cpudata[cpuid].status) &&
+		    adp_cpu_current[cpuid]->cpudata[cpuid].irq_pending_hi != 0)
+		    __adeos_sync_stage();
+		}
+
+	    break;
+	    }
+	else if (_adp == adp_cpu_current[cpuid])
+	    break;
+    }
+
+release_cpu_and_exit:
+
+    adeos_put_cpu(flags);
+}
+
+/* adeos_catch_event_from() -- Interpose an event handler starting
+   from a given domain. */
+
+int adeos_catch_event_from (adomain_t *adp, unsigned event, void (*handler)(adevinfo_t *))
+
+{
+    if (event >= ADEOS_NR_EVENTS)
+	return -EINVAL;
+
+    if (!xchg(&adp->events[event].handler,handler))
+	{
+	if (handler)
+	    __adeos_event_monitors[event]++;
+	}
+    else if (!handler)
+	__adeos_event_monitors[event]--;
+
+   return 0;
+}
+
+void (*adeos_hook_dswitch(void (*handler)(void))) (void) {
+
+    return (void (*)(void))xchg(&adp_current->dswitch,handler);
+}
+
+void adeos_init_attr (adattr_t *attr)
+
+{
+    attr->name = "Anonymous";
+    attr->domid = 1;
+    attr->entry = NULL;
+    attr->estacksz = 0;	/* Let ADEOS choose a reasonable stack size */
+    attr->priority = ADEOS_ROOT_PRI;
+    attr->dswitch = NULL;
+    attr->nptdkeys = 0;
+    attr->ptdset = NULL;
+    attr->ptdget = NULL;
+}
+
+int adeos_alloc_ptdkey (void)
+
+{
+    unsigned long flags;
+    int key = -1;
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (adp_current->ptd_keycount < adp_current->ptd_keymax)
+	{
+	key = ffz(adp_current->ptd_keymap);
+	set_bit(key,&adp_current->ptd_keymap);
+	adp_current->ptd_keycount++;
+	}
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return key;
+}
+
+int adeos_free_ptdkey (int key)
+
+{
+    unsigned long flags;
+
+    if (key < 0 || key >= adp_current->ptd_keymax)
+	return -EINVAL;
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (test_and_clear_bit(key,&adp_current->ptd_keymap))
+	adp_current->ptd_keycount--;
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+    return 0;
+}
+
+int adeos_set_ptd (int key, void *value)
+
+{
+    if (key < 0 || key >= adp_current->ptd_keymax)
+	return -EINVAL;
+
+    if (!adp_current->ptd_setfun)
+	{
+	printk(KERN_WARNING "Adeos: No ptdset hook for %s\n",adp_current->name);
+	return -EINVAL;
+	}
+
+    adp_current->ptd_setfun(key,value);
+
+    return 0;
+}
+
+void *adeos_get_ptd (int key)
+
+{
+    if (key < 0 || key >= adp_current->ptd_keymax)
+	return NULL;
+
+    if (!adp_current->ptd_getfun)
+	{
+	printk(KERN_WARNING "Adeos: No ptdget hook for %s\n",adp_current->name);
+	return NULL;
+	}
+
+    return adp_current->ptd_getfun(key);
+}
+
+int adeos_init_mutex (admutex_t *mutex)
+
+{
+    admutex_t initm = ADEOS_MUTEX_UNLOCKED;
+    *mutex = initm;
+    return 0;
+}
+
+int adeos_destroy_mutex (admutex_t *mutex)
+
+{
+    if (!adeos_spin_trylock(&mutex->lock) &&
+	adp_current != adp_root &&
+	mutex->owner != adp_current)
+	return -EBUSY;
+
+    return 0;
+}
+
+static inline void __adeos_sleepon_mutex (admutex_t *mutex, adomain_t *sleeper, int cpuid)
+
+{
+    adomain_t *owner = mutex->owner;
+
+    /* Make the current domain (== sleeper) wait for the mutex to be
+       released. Adeos' pipelined scheme guarantees that the new
+       sleeper _is_ more prioritary than any aslept domain since we
+       have stalled each sleeper's stage. Must be called with local hw
+       interrupts off. */
+
+    sleeper->m_link = mutex->sleepq;
+    mutex->sleepq = sleeper;
+    __adeos_switch_to(owner,cpuid);
+    mutex->owner = sleeper;
+    adeos_spin_unlock(&mutex->lock);
+}
+
+static inline void __adeos_signal_mutex (admutex_t *mutex, int cpuid)
+
+{
+    adomain_t *sleeper;
+
+    /* Wake up first most prioritary sleeper. Must be called with
+       local hw interrupts off. */
+
+    sleeper = mutex->sleepq;
+    mutex->sleepq = sleeper->m_link;
+    __adeos_switch_to(sleeper,cpuid);
+}
+
+unsigned long adeos_lock_mutex (admutex_t *mutex)
+
+{
+    unsigned long flags, hwflags;
+    adeos_declare_cpuid;
+    adomain_t *adp;
+
+    if (unlikely(!adp_pipelined))
+	{
+	adeos_hw_local_irq_save(hwflags);
+	flags = adeos_hw_test_iflag(hwflags);
+	adeos_spin_lock(&mutex->lock);
+	return flags;
+	}
+
+    adeos_lock_cpu(hwflags);
+
+    adp = adp_cpu_current[cpuid];
+
+    flags = test_and_set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+
+    /* Two cases to handle here on SMP systems, only one for UP:
+       1) in case of a conflicting access from a prioritary domain
+       running on the same cpu, make this domain sleep on the mutex,
+       and resume the current owner so it can release the lock asap.
+       2) in case of a conflicting access from any domain on a
+       different cpu than the current owner's, simply enter a spinning
+       loop. Note that testing mutex->owncpu is safe since it is only
+       changed by the current owner, and set to -1 when the mutex is
+       unlocked. */
+
+#ifdef CONFIG_SMP
+    while (!adeos_spin_trylock(&mutex->lock))
+	{
+	if (mutex->owncpu == cpuid)
+	    {
+	    __adeos_sleepon_mutex(mutex,adp,cpuid);
+	    adeos_load_cpuid();
+	    }
+	}
+
+    mutex->owncpu = cpuid;
+#else  /* !CONFIG_SMP */
+    while (mutex->owner != NULL && mutex->owner != adp)
+	__adeos_sleepon_mutex(mutex,adp,cpuid);
+#endif /* CONFIG_SMP */
+
+    mutex->owner = adp;
+
+    adeos_unlock_cpu(hwflags);
+
+    return flags;
+}
+
+void adeos_unlock_mutex (admutex_t *mutex, unsigned long flags)
+
+{
+    unsigned long hwflags;
+    adeos_declare_cpuid;
+    adomain_t *adp;
+
+    if (unlikely(!adp_pipelined))
+	{
+	adeos_spin_unlock(&mutex->lock);
+
+	if (flags)
+	    adeos_hw_cli();
+	else
+	    adeos_hw_sti();
+
+	return;
+	}
+
+#ifdef CONFIG_SMP
+    mutex->owncpu = -1;
+#endif /* CONFIG_SMP */
+
+    if (!flags)
+	adeos_hw_sti();	/* Absolutely needed. */
+
+    adeos_lock_cpu(hwflags);
+
+    if (unlikely(mutex->sleepq != NULL))
+	{
+	__adeos_signal_mutex(mutex,cpuid); /* Wake up one sleeper. */
+	adeos_load_cpuid();
+	}
+    else
+	{
+	mutex->owner = NULL;
+	adeos_spin_unlock(&mutex->lock);
+	}
+
+    adp = adp_cpu_current[cpuid];
+
+    if (flags)
+	set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    else
+	{
+	__clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+
+	if (adp->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+	}
+
+    adeos_unlock_cpu(hwflags);
+}
+
+void __adeos_takeover (void)
+
+{
+    __adeos_enable_pipeline();
+    printk(KERN_WARNING "Adeos:Pipelining started.\n");
+}
+
+#ifdef MODULE
+
+static int __init adeos_init_module (void)
+
+{
+     adeos_declare_cpuid;
+   __adeos_takeover();
+    return 0;
+}
+
+static void __exit adeos_exit_module (void)
+
+{
+   __adeos_disable_pipeline();
+    printk(KERN_WARNING "Adeos: Pipelining stopped.\n");
+}
+
+module_init(adeos_init_module);
+module_exit(adeos_exit_module);
+
+#endif /* MODULE */
+
+EXPORT_SYMBOL(adeos_register_domain);
+EXPORT_SYMBOL(adeos_unregister_domain);
+EXPORT_SYMBOL(adeos_renice_domain);
+EXPORT_SYMBOL(adeos_virtualize_irq_from);
+EXPORT_SYMBOL(adeos_control_irq);
+EXPORT_SYMBOL(adeos_propagate_irq);
+EXPORT_SYMBOL(adeos_schedule_irq);
+EXPORT_SYMBOL(adeos_free_irq);
+EXPORT_SYMBOL(adeos_trigger_ipi);
+EXPORT_SYMBOL(adeos_stall_pipeline);
+EXPORT_SYMBOL(adeos_unstall_pipeline);
+EXPORT_SYMBOL(adeos_unstall_pipeline_from);
+EXPORT_SYMBOL(adeos_catch_event_from);
+EXPORT_SYMBOL(adeos_hook_dswitch);
+EXPORT_SYMBOL(adeos_init_attr);
+EXPORT_SYMBOL(adeos_get_sysinfo);
+EXPORT_SYMBOL(adeos_tune_timer);
+EXPORT_SYMBOL(adeos_alloc_ptdkey);
+EXPORT_SYMBOL(adeos_free_ptdkey);
+EXPORT_SYMBOL(adeos_set_ptd);
+EXPORT_SYMBOL(adeos_get_ptd);
+EXPORT_SYMBOL(adeos_set_irq_affinity);
+EXPORT_SYMBOL(adeos_init_mutex);
+EXPORT_SYMBOL(adeos_destroy_mutex);
+EXPORT_SYMBOL(adeos_lock_mutex);
+EXPORT_SYMBOL(adeos_unlock_mutex);
+
+
diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 18e01f2..901f86e 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -572,7 +572,7 @@ config LEDS
 	  system, but the driver will do nothing.

 config LEDS_TIMER
-	bool "Timer LED" if LEDS && (ARCH_NETWINDER || ARCH_EBSA285 || ARCH_SHARK || MACH_MAINSTONE || ARCH_CO285 || ARCH_SA1100 || ARCH_LUBBOCK || ARCH_PXA_IDP || ARCH_INTEGRATOR || ARCH_P720T || ARCH_VERSATILE_PB)
+	bool "Timer LED" if LEDS && (ARCH_NETWINDER || ARCH_EBSA285 || ARCH_SHARK || MACH_MAINSTONE || ARCH_CO285 || ARCH_SA1100 || ARCH_LUBBOCK || ARCH_PXA_IDP || ARCH_INTEGRATOR || ARCH_P720T || ARCH_VERSATILE_PB || ARCH_IMX)
 	depends on ARCH_NETWINDER || ARCH_EBSA110 || ARCH_EBSA285 || ARCH_FTVPCI || ARCH_SHARK || ARCH_CO285 || ARCH_SA1100 || ARCH_LUBBOCK || MACH_MAINSTONE || ARCH_PXA_IDP || ARCH_INTEGRATOR || ARCH_CDB89712 || ARCH_P720T || ARCH_OMAP || ARCH_VERSATILE_PB || ARCH_IMX
 	default y if ARCH_EBSA110
 	help
@@ -836,3 +836,5 @@ source "crypto/Kconfig"

 source "lib/Kconfig"

+source "adeos/Kconfig"
+
diff --git a/arch/arm/Makefile b/arch/arm/Makefile
index eb6c8f3..c32a1fd 100644
--- a/arch/arm/Makefile
+++ b/arch/arm/Makefile
@@ -55,8 +55,11 @@ tune-$(CONFIG_CPU_XSCALE)	:=$(call check_gcc,-mtune=xscale,-mtune=strongarm110)
 tune-$(CONFIG_CPU_V6)		:=-mtune=strongarm

 # Need -Uarm for gcc < 3.x
-CFLAGS		+=-mapcs-32 $(arch-y) $(tune-y) -mshort-load-bytes -Wa,-mno-fpu -Uarm
-AFLAGS		+=-mapcs-32 $(arch-y) $(tune-y) -Wa,-mno-fpu
+#CFLAGS		+=-mapcs-32 $(arch-y) $(tune-y) -mshort-load-bytes -Wa,-mno-fpu -Uarm
+#AFLAGS         +=-mapcs-32 $(arch-y) $(tune-y) -Wa,-mno-fpu
+# gcc-3.4.x
+CFLAGS		+=-mapcs-32 $(arch-y) $(tune-y) -malignment-traps
+AFLAGS		+=-mapcs-32 $(arch-y) $(tune-y)

 #Default value
 DATAADDR	:= .
diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
index b8be933..273d88e 100644
--- a/arch/arm/kernel/Makefile
+++ b/arch/arm/kernel/Makefile
@@ -11,6 +11,7 @@ obj-y		:= arch.o compat.o dma.o entry-armv.o entry-common.o irq.o   \
 		   time.o traps.o

 obj-$(CONFIG_APM)		+= apm.o
+obj-$(CONFIG_ADEOS_CORE)	+= adeos.o
 obj-$(CONFIG_ARCH_ACORN)	+= ecard.o time-acorn.o
 obj-$(CONFIG_ARCH_CLPS7500)	+= time-acorn.o
 obj-$(CONFIG_FOOTBRIDGE)	+= isa.o
diff --git a/arch/arm/kernel/adeos.c b/arch/arm/kernel/adeos.c
new file mode 100644
index 0000000..191c82a
--- /dev/null
+++ b/arch/arm/kernel/adeos.c
@@ -0,0 +1,546 @@
+/*
+ *   linux/arch/armnommu/kernel/adeos.c
+ *
+ *   Copyright (C) 2003,2004 Philippe Gerum.
+ *   RTAI/ARM over Adeos rewrite for 255_2.6.7:
+ *   Copyright (c) 2005 Stefano Gafforelli (stefano.gafforelli@tiscali.it)
+ *   Copyright (c) 2005 Luca Pizzi (lucapizzi@hotmail.com)
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-dependent ADEOS core support for ARM.
+ */
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/bitops.h>
+#include <asm/system.h>
+#include <asm/atomic.h>
+#include <asm/irq.h>
+#include <asm-arm/adeos.h>
+#include <asm/io.h>
+#include <asm/arch/irq.h>
+#include <asm-arm/errno.h>
+#include <linux/module.h>
+
+int mio_irq;
+EXPORT_SYMBOL(mio_irq);
+
+asmlinkage void asm_do_IRQ(int irq,
+		       struct pt_regs *regs);
+
+extern struct list_head __adeos_pipeline;
+
+#ifdef CONFIG_ADEOS_MODULE
+
+/* A global flag telling whether Adeos pipelining is engaged. */
+int adp_pipelined = 0;
+
+#endif /* CONFIG_ADEOS_MODULE */
+
+struct pt_regs __adeos_irq_regs;
+
+#define ffnz(x) (ffs(x) - 1)
+
+#if 0
+void call_imx_timer_interrupt(int irq, void *dev_id, struct pt_regs *regs){
+	imx_timer_interrupt(irq, dev_id, regs);    // for imx
+}
+EXPORT_SYMBOL(call_imx_timer_interrupt);
+#endif
+
+/* adeos_critical_enter() -- Grab the superlock for entering a global
+   critical section. On this uniprocessor-only arch, this is identical
+   to hw cli(). */
+
+unsigned long adeos_critical_enter (void (*syncfn)(void))
+
+{
+    unsigned long flags;
+    adeos_hw_local_irq_save(flags);
+    return flags;
+}
+
+/* adeos_critical_exit() -- Release the superlock. */
+
+void adeos_critical_exit (unsigned long flags) {
+
+    adeos_hw_local_irq_restore(flags);
+}
+
+void __adeos_macro_disable_irq (void)
+
+{
+
+    if (likely(adp_pipelined))
+	{
+		__set_bit(IPIPE_STALL_FLAG, &adp_root->cpudata[adeos_processor_id()].status);
+	}
+}
+
+void  __adeos_macro_enable_irq (void)
+
+{
+
+       if (likely(adp_pipelined))
+	{
+	__clear_bit(IPIPE_STALL_FLAG, &adp_root->cpudata[adeos_processor_id()].status);
+ 	}
+}
+
+void __adeos_init_stage (adomain_t *adp)
+
+{
+    int cpuid, n;
+
+    for (cpuid = 0; cpuid < ADEOS_NR_CPUS; cpuid++)
+	{
+	adp->cpudata[cpuid].irq_pending_hi = 0;
+
+	for (n = 0; n < IPIPE_IRQ_IWORDS; n++)
+	    adp->cpudata[cpuid].irq_pending_lo[n] = 0;
+
+	for (n = 0; n < IPIPE_NR_IRQS; n++)
+	    adp->cpudata[cpuid].irq_hits[n] = 0;
+	}
+
+    for (n = 0; n < IPIPE_NR_IRQS; n++)
+	{
+	adp->irqs[n].acknowledge = NULL;
+	adp->irqs[n].handler = NULL;
+	adp->irqs[n].control = IPIPE_PASS_MASK;	/* Pass but don't handle */
+	}
+}
+EXPORT_SYMBOL(__adeos_init_stage);
+
+/* __adeos_sync_stage() -- Flush the pending IRQs for the current
+   domain (and processor).  This routine flushes the interrupt log
+   (see "Optimistic interrupt protection" from D. Stodolsky et al. for
+   more on the deferred interrupt scheme). Every interrupt that
+   occurred while the pipeline was stalled gets played.  WARNING:
+   callers on SMP boxen should always check for CPU migration on
+   return of this routine. */
+
+void __adeos_sync_stage(void)
+
+{
+    unsigned long mask, submask, flags;
+    struct adcpudata *cpudata;
+    adeos_declare_cpuid;
+    int level, rank;
+    adomain_t *adp;
+    int irq;
+
+    adeos_lock_cpu(flags);
+
+    adp = adp_cpu_current[cpuid];
+    cpudata = &adp->cpudata[cpuid];
+    do
+	{
+	if (unlikely(test_and_set_bit(IPIPE_SYNC_FLAG,&cpudata->status)))
+	    goto release_cpu_and_exit;
+
+	/* The policy here is to keep the dispatching code
+	   interrupt-free by stalling the current stage. If the upper
+	   domain handler (which we call) wants to re-enable
+	   interrupts while in a safe portion of the code
+	   (e.g. SA_INTERRUPT flag unset for Linux's sigaction()), it
+	   will have to unstall (then stall again before returning to
+	   us!) the stage when it sees fit. */
+
+	while ((mask = cpudata->irq_pending_hi) != 0)
+	    {
+	    /* Give a slight priority advantage to high-numbered IRQs
+	       like the virtual ones. */
+	    level = ffnz(mask);
+	    __clear_bit(level,&cpudata->irq_pending_hi);
+
+	    while ((submask = cpudata->irq_pending_lo[level]) != 0)
+		{
+		rank = ffnz(submask);
+		irq = (level << IPIPE_IRQ_ISHIFT) + rank;
+
+		if (unlikely(test_bit(IPIPE_LOCK_FLAG,&adp->irqs[irq].control)))
+		    {
+		    __clear_bit(rank,&cpudata->irq_pending_lo[level]);
+		    continue;
+		    }
+
+		if (likely(--cpudata->irq_hits[irq] == 0))
+		    __clear_bit(rank,&cpudata->irq_pending_lo[level]);
+
+		/* Allow the sync routine to be reentered on behalf of
+		   the IRQ handler and any execution context switched
+		   in by the IRQ handler. The latter also means that
+		   returning from the switched out context is always
+		   safe even if the sync routine has been reentered in
+		   the meantime. */
+		set_bit(IPIPE_STALL_FLAG,&cpudata->status);
+
+		__clear_bit(IPIPE_SYNC_FLAG,&cpudata->status);
+
+		adeos_unlock_cpu(flags);
+
+		adeos_hw_sti();
+
+		/* 1. __adeos_irq_regs is only overwritten when a
+		   timer IRQ is caught.
+		   2. IPIPE_CALLASM_FLAG is dummy for this arch. */
+		((void (*)(unsigned, struct pt_regs *))adp->irqs[irq].handler)(irq,&__adeos_irq_regs);
+		adeos_lock_cpu(flags);
+
+		__clear_bit(IPIPE_STALL_FLAG,&cpudata->status);
+
+		if (test_and_set_bit(IPIPE_SYNC_FLAG,&cpudata->status))
+		    goto release_cpu_and_exit;
+		}
+	    }
+
+	__clear_bit(IPIPE_SYNC_FLAG,&cpudata->status);
+
+	}
+    while (cpudata->irq_pending_hi != 0);
+
+release_cpu_and_exit:
+
+    adeos_unlock_cpu(flags);
+
+}
+
+static inline void __adeos_walk_pipeline (struct list_head *pos, int cpuid)
+
+{
+
+    while (pos != &__adeos_pipeline)
+	{
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	if (test_bit(IPIPE_STALL_FLAG,&_adp->cpudata[cpuid].status))
+	    break; /* Stalled stage -- do not go further. */
+
+	if (_adp->cpudata[cpuid].irq_pending_hi != 0)
+	    {
+	    if (_adp == adp_cpu_current[cpuid])
+		__adeos_sync_stage();
+	    else
+		{
+		__adeos_switch_to(_adp,cpuid);
+
+		if (!test_bit(IPIPE_STALL_FLAG,&adp_cpu_current[cpuid]->cpudata[cpuid].status) &&
+		    adp_cpu_current[cpuid]->cpudata[cpuid].irq_pending_hi != 0)
+		    __adeos_sync_stage();
+		}
+
+
+	    break;
+	    }
+	else if (_adp == adp_cpu_current[cpuid])
+
+	    break;
+
+	pos = _adp->p_link.next;
+	}
+
+}
+
+/* __adeos_handle_irq() -- ADEOS' generic IRQ handler. An optimistic
+   interrupt protection log is maintained here for each
+   domain. Interrupts are off on entry. */
+
+asmlinkage int __adeos_handle_irq (int irq, struct pt_regs *regs)
+{
+    struct list_head *head, *pos;
+    adeos_declare_cpuid;
+    int m_ack, s_ack;
+    m_ack = irq & 0x100;
+    irq &= 0xff;
+
+    if (!adp_pipelined)
+	{
+	asm_do_IRQ(irq,regs);
+	return 1;
+	}
+
+    if (!adeos_virtual_irq_p(irq))
+    {
+	irq = fixup_irq(irq);
+	}
+
+    if (irq >= IPIPE_NR_IRQS)
+	{
+	printk(KERN_ERR "ADEOS: spurious interrupt %d\n",irq);
+	return 1;
+	}
+
+    adeos_load_cpuid();
+    s_ack = m_ack;
+
+    if (unlikely(test_bit(IPIPE_STICKY_FLAG,&adp_cpu_current[cpuid]->irqs[irq].control)))
+	head = &adp_cpu_current[cpuid]->p_link;
+    else
+	head = __adeos_pipeline.next;
+
+    /* Ack the interrupt. */
+
+    pos = head;
+
+    while (pos != &__adeos_pipeline)
+	{
+    	adomain_t *_adp = list_entry(pos,adomain_t,p_link);
+
+	/* For each domain handling the incoming IRQ, mark it as
+           pending in its log. */
+
+	if (test_bit(IPIPE_HANDLE_FLAG,&_adp->irqs[irq].control))
+	    {
+	    /* Domains that handle this IRQ are polled for
+	       acknowledging it by decreasing priority order. The
+	       interrupt must be made pending _first_ in the domain's
+	       status flags before the PIC is unlocked. */
+
+	    _adp->cpudata[cpuid].irq_hits[irq]++;
+	    __adeos_set_irq_bit(_adp,cpuid,irq);
+
+	    /* Always get the first master acknowledge available. Once2Change
+	       we've got it, allow slave acknowledge handlers to run
+	       (until one of them stops us). */
+
+	    if (!m_ack)
+		m_ack = _adp->irqs[irq].acknowledge(irq);
+	    else if (test_bit(IPIPE_SHARED_FLAG,&_adp->irqs[irq].control) && !s_ack)
+		s_ack = _adp->irqs[irq].acknowledge(irq);
+	    }
+
+	/* If the domain does not want the IRQ to be passed down the
+	   interrupt pipe, exit the loop now. */
+
+	if (!test_bit(IPIPE_PASS_FLAG,&_adp->irqs[irq].control))
+	    break;
+
+	pos = _adp->p_link.next;
+	}
+
+    if (likely(irq == ADEOS_TIMER_IRQ))
+	{
+	__adeos_irq_regs.ARM_cpsr = regs->ARM_cpsr;
+	__adeos_irq_regs.ARM_pc = regs->ARM_pc;
+	}
+
+    /* Now walk the pipeline, yielding control to the highest priority
+       domain that has pending interrupt(s) or immediately to the
+       current domain if the interrupt has been marked as
+       'sticky'. This search does not go beyond the current domain in
+       the pipeline. To understand this code properly, one must keep
+       in mind that domains having a higher priority than the current
+       one are sleeping on the adeos_suspend_domain() service. In
+       addition, domains having a lower priority have been preempted
+       by an interrupt dispatched to a more prioritary domain. Once
+       the first and most prioritary stage has been selected here, the
+       subsequent stages will be activated in turn when each visited
+       domain calls adeos_suspend_domain() to wake up its neighbour
+       down the pipeline. */
+
+    __adeos_walk_pipeline(head,cpuid);
+
+    return (adp_cpu_current[cpuid] == adp_root &&
+	    !test_bit(IPIPE_STALL_FLAG,&adp_root->cpudata[cpuid].status));
+}
+
+/* adeos_trigger_irq() -- Push the interrupt to the pipeline entry
+   just like if it has been actually received from a hw source. This
+   both works for real and virtual interrupts. This also means that
+   the current domain might be immediately preempted by a more
+   prioritary domain who happens to handle this interrupt. */
+
+int adeos_trigger_irq (unsigned irq)
+
+{
+    struct pt_regs regs;
+    if (irq >= IPIPE_NR_IRQS ||
+	(adeos_virtual_irq_p(irq) && !test_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map)))
+	return -EINVAL;
+
+    adeos_hw_local_irq_flags(regs.ARM_cpsr);
+
+    __adeos_handle_irq(irq | 0x100,&regs);
+
+    return 1;
+}
+
+/* default handler for Adeos syscall (will be replaced by non-root domain, e.g.
+ * RTAI for LXRT calls and SRQs */
+asmlinkage int __adeos_handle_syscall(struct pt_regs *regs)
+
+{
+
+    /* just return error code and do fast return from syscall */
+    *(long long*)&regs->ARM_r0 = -ENODEV;
+    return 1;
+}
+
+asmlinkage int __adeos_enter_syscall (struct pt_regs *regs, int scno)
+
+{
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SYSCALL_PROLOGUE] > 0))
+	{
+	long oldip = regs->ARM_ip;
+	int s;
+
+	regs->ARM_ip = scno;
+	s = __adeos_handle_event(ADEOS_SYSCALL_PROLOGUE,regs);
+	regs->ARM_ip = oldip;
+
+	return s;
+	}
+
+    return 0;
+}
+
+asmlinkage int __adeos_exit_syscall (void) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SYSCALL_EPILOGUE] > 0))
+	return __adeos_handle_event(ADEOS_SYSCALL_EPILOGUE,NULL);
+
+    return 0;
+}
+
+extern int (*adeos_syscall_entry)(struct pt_regs *regs);
+EXPORT_SYMBOL(adeos_syscall_entry);
+
+/**********************************/
+#ifdef CONFIG_ADEOS_CORE
+extern asmlinkage int do_notify_resume( struct pt_regs *regs, unsigned int thread_flags, int syscall);
+extern asmlinkage void syscall_trace(int why,  struct pt_regs *regs);
+asmlinkage void __adeos_wrap_schedule(void)
+{
+		schedule();
+}
+
+asmlinkage void __adeos_wrap_syscall_trace(int why, struct pt_regs *regs)
+{
+		syscall_trace(why, regs);
+}
+
+asmlinkage int __adeos_wrap_do_notify_resume( struct pt_regs *regs, unsigned int thread_flags, int syscall)
+{
+		return do_notify_resume(regs, thread_flags, syscall);
+}
+#if 0
+/*Possible interrupt consistency verifications taken from Guennadi's patch for linux 2.4.19 PXA
+*/
+asmlinkage void __adeos_wrap_schedule(void)
+{
+	if (!adp_pipelined) {
+		schedule();
+	}  else {
+		unsigned long flags;
+		unsigned long adeos_i_bit;
+
+		adeos_hw_local_irq_flags(flags);
+		adeos_i_bit = __adeos_test_root();
+		if ( flags & PSR_I_BIT) {
+			if (!adeos_i_bit) {
+				__adeos_stall_root();
+			}
+			adeos_hw_sti();
+		} else if (adeos_i_bit) {
+			__adeos_unstall_root();
+		}
+		schedule();
+		adeos_hw_local_irq_flags(flags);
+		if (flags & PSR_I_BIT) {
+			adeos_hw_sti();
+			BUG();
+		}
+		if (__adeos_test_root()) {
+			adeos_hw_cli();
+		}
+		__adeos_restore_root_nosync(adeos_i_bit);
+	}
+}
+
+asmlinkage void __adeos_wrap_syscall_trace(int why, struct pt_regs *regs)
+{
+	if (!adp_pipelined) {
+		syscall_trace(why, regs);
+	}  else {
+		unsigned long flags;
+		unsigned long adeos_i_bit;
+
+		adeos_hw_local_irq_flags(flags);
+		adeos_i_bit = __adeos_test_root();
+		if ( flags & PSR_I_BIT) {
+			if (!adeos_i_bit) {
+				__adeos_stall_root();
+			}
+			adeos_hw_sti();
+		} else if (adeos_i_bit) {
+			__adeos_unstall_root();
+		}
+		syscall_trace(why, regs);
+		adeos_hw_local_irq_flags(flags);
+		if (flags & PSR_I_BIT) {
+			adeos_hw_sti();
+			BUG();
+		}
+		if (__adeos_test_root()) {
+			adeos_hw_cli();
+		}
+		__adeos_restore_root_nosync(adeos_i_bit);
+	}
+}
+
+asmlinkage int __adeos_wrap_do_notify_resume( struct pt_regs *regs, unsigned int thread_flags, int syscall)
+{
+		int ret;
+	if (!adp_pipelined) {
+		return do_notify_resume(regs, thread_flags, syscall);
+	}  else {
+		unsigned long flags;
+		unsigned long adeos_i_bit;
+
+		adeos_hw_local_irq_flags(flags);
+		adeos_i_bit = __adeos_test_root();
+		if ( flags & PSR_I_BIT) {
+			if (!adeos_i_bit) {
+				__adeos_stall_root();
+			}
+			adeos_hw_sti();
+		} else if (adeos_i_bit) {
+			__adeos_unstall_root();
+		}
+		ret = do_notify_resume(regs, thread_flags, syscall);
+		adeos_hw_local_irq_flags(flags);
+		if (flags & PSR_I_BIT) {
+			adeos_hw_sti();
+			BUG();
+		}
+		if (__adeos_test_root()) {
+			adeos_hw_cli();
+		}
+		__adeos_restore_root_nosync(adeos_i_bit);
+	}
+	return ret;
+}
+#endif
+
+#endif
+
+
diff --git a/arch/arm/kernel/armksyms.c b/arch/arm/kernel/armksyms.c
index 43f0c66..3fdd03f 100644
--- a/arch/arm/kernel/armksyms.c
+++ b/arch/arm/kernel/armksyms.c
@@ -49,6 +49,39 @@ extern void fp_enter(void);
   __attribute__((section("__ksymtab"))) =	\
     { (unsigned long)&orig, #sym };

+#ifdef CONFIG_ADEOS_CORE
+#ifdef CONFIG_ADEOS_MODULE
+EXPORT_SYMBOL(adp_pipelined);
+#endif /* CONFIG_ADEOS_MODULE */
+EXPORT_SYMBOL(adeos_critical_enter);
+EXPORT_SYMBOL(adeos_critical_exit);
+EXPORT_SYMBOL(adeos_trigger_irq);
+EXPORT_SYMBOL(__adeos_sync_stage);
+EXPORT_SYMBOL(__adeos_irq_regs);
+EXPORT_SYMBOL(__adeos_tune_timer);
+EXPORT_SYMBOL(__adeos_switch_domain);
+EXPORT_SYMBOL(__adeos_handle_irq);
+
+/* Mail Neuhauser: Re: problem with rtai_lxrt module (arm-pxa255 kernel 2.6.7)
+ - 15 Feb 2005*/
+extern void cpu_arm920_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
+EXPORT_SYMBOL(cpu_arm920_switch_mm);
+
+/* The following are per-platform convenience exports which are needed
+   by some Adeos domains loaded as kernel modules. */
+extern void show_regs(struct pt_regs * regs);
+extern void __divdi3(void);
+/* __fixunsdfsi hack for gcc-3.4.6 */
+void __fixunsdfsi(void){return;};
+extern void __fixunsdfsi(void);
+extern struct irqdesc irq_desc[];
+EXPORT_SYMBOL_NOVERS(irq_desc);
+EXPORT_SYMBOL_NOVERS(__switch_to);
+EXPORT_SYMBOL_NOVERS(show_regs);
+EXPORT_SYMBOL_NOVERS(__divdi3);
+EXPORT_SYMBOL_NOVERS(__fixunsdfsi);
+#endif /* CONFIG_ADEOS_CORE */
+
 /*
  * floating point math emulator support.
  * These symbols will never change their calling convention...
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index efafb90..19e6088 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -385,6 +385,11 @@ ENTRY(soft_irq_mask)
 		ands	\irqstat, \irqstat, \irqnr
 		mov	\irqnr, #0
 		beq	1001f
+#ifdef CONFIG_ADEOS_CORE
+		tst     \irqstat, #0x04000000   @ check OSMR0 first
+		movne   \irqnr, #26
+		bne     1001f
+#endif /* CONFIG_ADEOS_CORE */
 		tst	\irqstat, #0xff
 		moveq	\irqstat, \irqstat, lsr #8
 		addeq	\irqnr, \irqnr, #8
@@ -1000,7 +1005,13 @@ __irq_svc:	sub	sp, sp, #S_FRAME_SIZE
 		@ routine called with r0 = irq number, r1 = struct pt_regs *
 		@
 		adrsvc	ne, lr, 1b
+
+#ifdef CONFIG_ADEOS_CORE
+		ldrne	pc, adeos_irq_entry		@ ...(r0 = irq number, r1 = struct pt_regs *)
+#else /* !CONFIG_ADEOS_CORE */
 		bne	asm_do_IRQ
+#endif /* CONFIG_ADEOS_CORE */
+
 #ifdef CONFIG_PREEMPT
 		ldr	r0, [r8, #TI_FLAGS]		@ get flags
 		tst	r0, #_TIF_NEED_RESCHED
@@ -1046,6 +1057,14 @@ __und_svc:	sub	sp, sp, #S_FRAME_SIZE
 		add	r2, sp, #S_SP
 		stmia	r2, {r3 - r7}			@ save sp_SVC, lr_SVC, pc, cpsr, old_ro

+#ifdef CONFIG_ADEOS_CORE
+		mov	r0, #4				@ SIGILL
+		mov	r1, sp				@ struct pt_regs *regs
+		bl	__adeos_handle_event
+		cmp	r0, #0				@ branch to root (Linux) trap handler
+		bne	1f				@ on null return.
+#endif /* CONFIG_ADEOS_CORE */
+
 		ldr	r0, [r5, #-4]			@ r0 = instruction
 		adrsvc	al, r9, 1f			@ r9 = normal FP return
 		bl	call_fpe			@ lr = undefined instr return
@@ -1091,6 +1110,12 @@ __pabt_svc:	sub	sp, sp, #S_FRAME_SIZE
 .LCirq_stat:	.word	irq_stat
 #endif

+#ifdef CONFIG_ADEOS_CORE
+		@ hook for RTAI immediate irq dispatching
+		.globl adeos_irq_entry
+adeos_irq_entry:		.word	__adeos_handle_irq	@ Adeos default irq handler
+#endif /* CONFIG_ADEOS_CORE */
+
 		irq_prio_table

 /*
@@ -1140,7 +1165,16 @@ __irq_usr:	sub	sp, sp, #S_FRAME_SIZE
 		@
 		@ routine called with r0 = irq number, r1 = struct pt_regs *
 		@
+
+#ifdef CONFIG_ADEOS_CORE
+ 		ldrne	pc, adeos_irq_entry
+		cmp	r0, #0
+		bne	 2f
+		slow_restore_user_regs
+#else /* !CONFIG_ADEOS_CORE */
 		bne	asm_do_IRQ
+#endif /* CONFIG_ADEOS_CORE */
+
 #ifdef CONFIG_PREEMPT
 		ldr	r0, [r8, #TI_PREEMPT]
 		teq	r0, r7
@@ -1148,7 +1182,8 @@ __irq_usr:	sub	sp, sp, #S_FRAME_SIZE
 		strne	r0, [r0, -r0]
 		mov	tsk, r8
 #else
-		get_thread_info tsk
+//		get_thread_info tsk
+2:  		get_thread_info tsk
 #endif
 		mov	why, #0
 		b	ret_to_user
@@ -1287,6 +1322,33 @@ ENTRY(__switch_to)
 		mcr	p15, 0, r3, c3, c0, 0		@ Set domain register
 		ldmib	r2, {r4 - sl, fp, sp, pc}	@ Load all regs saved previously

+#ifdef CONFIG_ADEOS_CORE
+/*
+ * Domain switch code. Interrupts must be off on entry.
+ * r0 = incoming domain, r1 = &adp_cpu_current[cpuid]
+ * Contributed by Jerome Poichet <jerome@kingofsofa.org>
+ */
+
+ENTRY(__adeos_switch_domain)
+	stmfd   sp!, {r0 - sl, fp, lr}  @ push most registers onto stack (scratch ip never)
+	mrs     ip, cpsr                @ get current cpsr_SVC and
+	str     ip, [sp, #-4]!          @ push it onto stack
+	mrc     p15, 0, r2, c3, c0      @ get current domain_access_control and
+	str     r2, [sp, #-4]!          @ push it onto stack
+
+	ldr     r2, [r1]                @ r2 = adp_cpu_current[cpuid] (outgoing domain)
+	str     sp, [r2, #4]            @ save outgoing sp_SVC
+	str     r0, [r1]                @ adp_cpu_current[cpuid] = incoming_domain
+	ldr     sp, [r0, #4]            @ load incoming sp_SVC
+
+	ldr     r2, [sp], #4            @ pop previous domain_access_control from stack
+	ldr     ip, [sp], #4            @ pop previous cpsr_SVC from stack
+	mcr     p15, 0, r2, c3, c0      @ restore previous domain_access_control
+	msr     spsr, ip                @ replace current spsr_SVC with previous cpsr_SVC
+	ldmfd   sp!, {r0 - sl, fp, pc}^ @ pop previous registers, pc = previous lr
+
+#endif /* CONFIG_ADEOS_CORE */
+
 		__INIT
 /*
  * Vector stubs.  NOTE that we only align 'vector_IRQ' to a cache line boundary,
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index 8686a90..8e01814 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -29,6 +29,14 @@
  * stack.
  */
 ret_fast_syscall:
+
+#ifdef CONFIG_ADEOS_CORE
+	str	r0, [sp, #-4]!			@ Save return code
+	bl	__adeos_exit_syscall
+	ldr	r0, [sp], #4
+__adeos_fast_ret:
+#endif /* CONFIG_ADEOS_CORE */
+
 	disable_irq r1				@ disable interrupts
 	ldr	r1, [tsk, #TI_FLAGS]
 	tst	r1, #_TIF_WORK_MASK
@@ -47,12 +55,21 @@ work_pending:
 	beq	no_work_pending
 	mov	r0, sp				@ 'regs'
 	mov	r2, why				@ 'syscall'
+
+#ifdef CONFIG_ADEOS_CORE
+	bl	__adeos_wrap_do_notify_resume
+#else
 	bl	do_notify_resume
+#endif
 	disable_irq r1				@ disable interrupts
 	b	no_work_pending

 work_resched:
+#ifdef CONFIG_ADEOS_CORE
+	bl	__adeos_wrap_schedule
+#else
 	bl	schedule
+#endif
 /*
  * "slow" syscall return path.  "why" tells us if this was a real syscall.
  */
@@ -77,7 +94,11 @@ ENTRY(ret_from_fork)
 	beq	ret_slow_syscall
 	mov	r1, sp
 	mov	r0, #1				@ trace exit [IP = 1]
+#ifdef CONFIG_ADEOS_CORE
+	bl	__adeos_wrap_syscall_trace
+#else
 	bl	syscall_trace
+#endif
 	b	ret_slow_syscall


@@ -130,9 +151,34 @@ ENTRY(vector_swi)
 	str	r4, [sp, #-S_OFF]!		@ push fifth arg

 	get_thread_info tsk
+
+/* #ifdef CONFIG_ADEOS_CORE
+	stmfd	sp!, {r0-r3, ip}
+	add	r0, sp, #(4*4 + S_OFF)
+	mov	r1, scno
+	bl	__adeos_enter_syscall
+	cmp     r0,#0
+	ldmfd	sp!, {r0-r3, ip}
+	bne	__adeos_fast_ret
+#endif */
+
 	ldr	ip, [tsk, #TI_FLAGS]		@ check for syscall tracing
 	bic	scno, scno, #0xff000000		@ mask off SWI op-code
+#ifdef CONFIG_ADEOS_CORE
+	ldr	tbl, .__adeos_usr2kern_mode
+	cmp	scno, tbl			@ (use tbl for scratch)
+	beq	4f
+#endif
 	eor	scno, scno, #OS_NUMBER << 20	@ check OS number
+#ifdef CONFIG_ADEOS_CORE
+	stmfd	sp!, {r0-r3, ip}
+	add	r0, sp, #S_OFF
+	mov	r1, scno
+	bl	__adeos_enter_syscall
+	cmp     r0,#0
+	ldmfd	sp!, {r0-r3, ip}
+	bne	__adeos_fast_ret
+#endif  /* CONFIG_ADEOS_CORE */
 	adr	tbl, sys_call_table		@ load syscall table pointer
 	tst	ip, #_TIF_SYSCALL_TRACE		@ are we tracing syscalls?
 	bne	__sys_trace
@@ -148,6 +194,39 @@ ENTRY(vector_swi)
 	bcs	arm_syscall
 	b	sys_ni_syscall			@ not private func

+#if 0
+#ifdef CONFIG_ADEOS_CORE
+4: 	ldr	r7, adeos_syscall_entry
+	mov	lr, pc                  @ call ...
+	mov	pc, r7			@ ... if available
+	add	sp, sp, #S_OFF		@ hmm, leave anyway
+	cmp	r0 , #0
+	bne		__adeos_fast_ret
+	b		work_pending
+.__adeos_usr2kern_mode:	.word	0x404404
+#endif
+#endif
+
+#ifdef CONFIG_ADEOS_CORE
+	@ handle Adeos syscall (used by RTAI for LXRT calls and SRQs)
+4:	ldr	ip, adeos_syscall_entry
+	add	r0, sp, #S_OFF			@ r0 <- ptr to saved regs
+	mov	lr, pc
+	mov	pc, ip				@ adeos_syscall_entry(pt_regs*)
+	cmp	r0 , #0				@ slow or fast return?
+	@ long long return value is in saved r0/r1, r0 is not loaded from stack
+	@ by fast_restore_user_regs => load it here.
+	ldrne	r0, [sp, #S_R0 + S_OFF]
+	disable_irq ip				@ needed in both cases! (i.e. slow + fast return)
+	@ r0 != 0 -> fast
+	bne	__adeos_fast_ret
+	@ r0 == 0 -> slow return
+	add	sp, sp, #S_OFF			@ fix stack-pointer for restore_user_regs
+	mov	why, #0				@ no longer a real syscall
+	b	ret_to_user
+.__adeos_usr2kern_mode:	.word	0x404404
+#endif /* CONFIG_ADEOS_CORE */
+
 	/*
 	 * This is the really slow path.  We're going to be doing
 	 * context switches, and waiting for our parent to respond.
@@ -155,8 +234,12 @@ ENTRY(vector_swi)
 __sys_trace:
 	add	r1, sp, #S_OFF
 	mov	r0, #0				@ trace entry [IP = 0]
-	bl	syscall_trace

+#ifdef CONFIG_ADEOS_CORE
+	bl	__adeos_wrap_syscall_trace
+#else
+	bl	syscall_trace
+#endif
 	adrsvc	al, lr, __sys_trace_return	@ return address
 	add	r1, sp, #S_R0 + S_OFF		@ pointer to regs
 	cmp	scno, #NR_syscalls		@ check upper syscall limit
@@ -168,7 +251,11 @@ __sys_trace_return:
 	str	r0, [sp, #S_R0 + S_OFF]!	@ save returned r0
 	mov	r1, sp
 	mov	r0, #1				@ trace exit [IP = 1]
+#ifdef CONFIG_ADEOS_CORE
+	bl	__adeos_wrap_syscall_trace
+#else
 	bl	syscall_trace
+#endif
 	b	ret_slow_syscall

 	.align	5
@@ -178,6 +265,13 @@ __cr_alignment:
 	.word	cr_alignment
 #endif

+#ifdef CONFIG_ADEOS_CORE
+	.word	__adeos_enter_syscall
+	.word	__adeos_exit_syscall
+	.globl adeos_syscall_entry
+adeos_syscall_entry:	.word	__adeos_handle_syscall	@ Adeos default syscall handler
+#endif /* CONFIG_ADEOS_CORE */
+
 	.type	sys_call_table, #object
 ENTRY(sys_call_table)
 #include "calls.S"
diff --git a/arch/arm/kernel/irq.c b/arch/arm/kernel/irq.c
index 5777a8c..31b63cb 100644
--- a/arch/arm/kernel/irq.c
+++ b/arch/arm/kernel/irq.c
@@ -327,6 +327,9 @@ do_edge_IRQ(unsigned int irq, struct irqdesc *desc, struct pt_regs *regs)
 	/*
 	 * Acknowledge and clear the IRQ, but don't mask it.
 	 */
+#ifdef CONFIG_ADEOS_CORE
+    if (!adp_pipelined)
+#endif /* CONFIG_ADEOS_CORE */
 	desc->chip->ack(irq);

 	/*
@@ -470,8 +473,13 @@ asmlinkage void asm_do_IRQ(unsigned int irq, struct pt_regs *regs)

 	spin_unlock(&irq_controller_lock);
 	irq_exit();
+
+	if ((adp_pipelined) && (irq == 13))
+		desc->chip->unmask(irq);
 }

+EXPORT_SYMBOL(asm_do_IRQ);
+
 void __set_irq_handler(unsigned int irq, irq_handler_t handle, int is_chained)
 {
 	struct irqdesc *desc;
diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
index 8d9db74..9bba5fa 100644
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@ -102,6 +102,10 @@ void cpu_idle(void)
 		void (*idle)(void) = pm_idle;
 		if (!idle)
 			idle = default_idle;
+#ifdef CONFIG_ADEOS_CORE
+		adeos_suspend_domain();
+#endif /* CONFIG_ADEOS_CORE */
+
 		preempt_disable();
 		leds_event(led_idle_start);
 		while (!need_resched())
diff --git a/arch/arm/kernel/time.c b/arch/arm/kernel/time.c
index bfed0f4..6906274 100644
--- a/arch/arm/kernel/time.c
+++ b/arch/arm/kernel/time.c
@@ -46,6 +46,8 @@ spinlock_t rtc_lock = SPIN_LOCK_UNLOCKED;
 EXPORT_SYMBOL(rtc_lock);
 #endif

+#define TIMER_BASE IMX_TIM1_BASE
+
 /* change this if you have some constant time drift */
 #define USECS_PER_JIFFY	(1000000/HZ)

@@ -324,4 +326,47 @@ static struct irqaction timer_irq = {
 /*
  * Include architecture specific code
  */
+
 #include <asm/arch/time.h>
+
+#ifdef CONFIG_ADEOS_CORE
+inline void __adeos_set_timer (unsigned long hz)
+{
+	volatile TimerStruct_t *timer0 =
+		(volatile TimerStruct_t *) TIMER0_VA_BASE;
+	volatile TimerStruct_t *timer1 =
+		(volatile TimerStruct_t *) TIMER1_VA_BASE;
+
+	timer_irq.handler = imx_timer_interrupt;
+
+	/*
+	 * Initialise to a known state (all timers off, and timing reset)
+	 */
+	timer0->TimerControl = 0;
+	timer1->TimerControl = 0;
+	timer0->TimerPrescaler = 0;
+	timer1->TimerPrescaler = 0;
+
+	timer0->TimerCompare = 328;
+	timer0->TimerControl = (TIM_32KHZ | TIM_INTEN | TIM_ENAB);
+
+	/*
+	 * Make irqs happen for the system timer
+	 */
+	setup_irq(TIM1_INT, &timer_irq);
+	gettimeoffset = imx_gettimeoffset;
+}
+
+#endif /* CONFIG_ADEOS_CORE */
+
+#ifdef CONFIG_ADEOS_CORE
+
+/* Simply inline the arch-specific code to set the frequency of the
+   timer. Interrrupts must be masked on entry. */
+
+void __adeos_tune_timer (unsigned long hz)
+{
+	__adeos_set_timer(hz);
+}
+
+#endif /* CONFIG_ADEOS_CORE */
diff --git a/arch/arm/lib/udivdi3.c b/arch/arm/lib/udivdi3.c
index d25195f..af53363 100644
--- a/arch/arm/lib/udivdi3.c
+++ b/arch/arm/lib/udivdi3.c
@@ -240,3 +240,43 @@ __umoddi3 (UDItype u, UDItype v)
   return w;
 }

+/* added signed variant for RTAI */
+
+static inline DItype
+__negdi2 (DItype u)
+{
+  DIunion w;
+  DIunion uu;
+
+  uu.ll = u;
+
+  w.s.low = -uu.s.low;
+  w.s.high = -uu.s.high - ((USItype) w.s.low > 0);
+
+  return w.ll;
+}
+
+DItype
+__divdi3 (DItype u, DItype v)
+{
+  word_type c = 0;
+  DIunion uu, vv;
+  DItype w;
+
+  uu.ll = u;
+  vv.ll = v;
+
+  if (uu.s.high < 0)
+    c = ~c,
+    uu.ll = __negdi2 (uu.ll);
+  if (vv.s.high < 0)
+    c = ~c,
+    vv.ll = __negdi2 (vv.ll);
+
+  w = __udivmoddi4 (uu.ll, vv.ll, (UDItype *) 0);
+  if (c)
+    w = __negdi2 (w);
+
+  return w;
+}
+
diff --git a/arch/arm/mach-imx/irq.c b/arch/arm/mach-imx/irq.c
index ead7141..297851a 100644
--- a/arch/arm/mach-imx/irq.c
+++ b/arch/arm/mach-imx/irq.c
@@ -161,6 +161,7 @@ imx_gpiob_demux_handler(unsigned int irq_unused, struct irqdesc *desc,
 	mask = ISR(1);
 	irq = IRQ_GPIOB(0);
 	imx_gpio_handler(mask, irq, desc, regs);
+	imx_unmask_irq(irq);
 }

 static void
diff --git a/arch/arm/mach-imx/time.c b/arch/arm/mach-imx/time.c
index 8103489..06904ff 100644
--- a/arch/arm/mach-imx/time.c
+++ b/arch/arm/mach-imx/time.c
@@ -12,6 +12,7 @@
 #include <linux/sched.h>
 #include <linux/init.h>

+#include <asm/system.h>
 #include <asm/hardware.h>
 #include <asm/io.h>

@@ -42,5 +43,5 @@ imx_rtc_init(void)

 	return 0;
 }
-
 __initcall(imx_rtc_init);
+
diff --git a/include/asm-arm/adeos.h b/include/asm-arm/adeos.h
new file mode 100644
index 0000000..2a42a8f
--- /dev/null
+++ b/include/asm-arm/adeos.h
@@ -0,0 +1,314 @@
+/*
+ *   include/asm-arm/adeos.h
+ *
+ *   Copyright (C) 2002,2003,2004 Philippe Gerum.
+ *   RTAI/ARM over Adeos rewrite for PXA255_2.6.7:
+ *   Copyright (c) 2005 Stefano Gafforelli (stefano.gafforelli@tiscali.it)
+ *   Copyright (c) 2005 Luca Pizzi (lucapizzi@hotmail.com)
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+
+#ifndef __ARM_ADEOS_H
+#define __ARM_ADEOS_H
+
+struct task_struct;
+
+#include <asm/irq.h>
+#include <asm/siginfo.h>
+#include <asm/ptrace.h>
+#include <asm/bitops.h>
+#include <linux/list.h>
+#include <linux/threads.h>
+#include <asm/arch/hardware.h>
+#include <asm/arch/irqs.h>
+
+#define ADEOS_ARCH_STRING "arm-iMX"
+#define ADEOS_TIMER_IRQ TIM1_INT
+
+#ifdef CONFIG_ADEOS_MODULE
+extern int adp_pipelined;
+#else  /* !CONFIG_ADEOS_MODULE */
+#define adp_pipelined 1		/* Testing this should be optimized out. */
+#endif /* CONFIG_ADEOS_MODULE */
+
+#define ADEOS_NR_CPUS          1
+#define adeos_processor_id()   0
+/* Array references using this index should be optimized out. */
+#define adeos_declare_cpuid    const int cpuid = 0
+#define adeos_load_cpuid()      /* nop */
+#define adeos_lock_cpu(flags)   adeos_hw_local_irq_save(flags)
+#define adeos_unlock_cpu(flags) adeos_hw_local_irq_restore(flags)
+#define adeos_get_cpu(flags)    do { flags = flags; } while(0)
+#define adeos_put_cpu(flags)    /* nop */
+#define adp_current             (adp_cpu_current[0])
+
+
+/* ARM fault traps */
+#define ADEOS_NR_FAULTS         32
+/* Pseudo-vectors used for kernel events */
+#define ADEOS_FIRST_KEVENT      ADEOS_NR_FAULTS
+#define ADEOS_SYSCALL_PROLOGUE  (ADEOS_FIRST_KEVENT)
+#define ADEOS_SYSCALL_EPILOGUE  (ADEOS_FIRST_KEVENT + 1)
+#define ADEOS_SCHEDULE_HEAD     (ADEOS_FIRST_KEVENT + 2)
+#define ADEOS_SCHEDULE_TAIL     (ADEOS_FIRST_KEVENT + 3)
+#define ADEOS_ENTER_PROCESS     (ADEOS_FIRST_KEVENT + 4)
+#define ADEOS_EXIT_PROCESS      (ADEOS_FIRST_KEVENT + 5)
+#define ADEOS_SIGNAL_PROCESS    (ADEOS_FIRST_KEVENT + 6)
+#define ADEOS_KICK_PROCESS      (ADEOS_FIRST_KEVENT + 7)
+#define ADEOS_LAST_KEVENT       (ADEOS_KICK_PROCESS)
+#define ADEOS_RENICE_PROCESS    (ADEOS_FIRST_KEVENT + 8)
+#undef ADEOS_LAST_KEVENT
+#define ADEOS_LAST_KEVENT       (ADEOS_RENICE_PROCESS)
+#define ADEOS_USER_EVENT        (ADEOS_FIRST_KEVENT + 9)
+#undef ADEOS_LAST_KEVENT
+#define ADEOS_LAST_KEVENT       (ADEOS_USER_EVENT)
+
+#define ADEOS_NR_EVENTS         (ADEOS_LAST_KEVENT + 1)
+
+#define ADEOS_USR2KERN_MODE 			0x404404
+
+typedef struct adevinfo {
+
+    unsigned domid;
+    unsigned event;
+    void *evdata;
+
+    volatile int propagate;	/* Private */
+
+} adevinfo_t;
+
+typedef struct adsysinfo {
+
+    int ncpus;			/* Number of CPUs on board */
+
+    unsigned long long cpufreq;	/* CPU frequency (in Hz) */
+
+    /* Arch-dependent block */
+
+    struct {
+	unsigned tmirq;		/* Timer tick IRQ */
+    } archdep;
+
+} adsysinfo_t;
+
+#define IPIPE_NR_XIRQS   NR_IRQS
+/* Number of virtual IRQs */
+#define IPIPE_NR_VIRQS   32
+/* First virtual IRQ # */
+#define IPIPE_VIRQ_BASE   IPIPE_NR_XIRQS
+/* Total number of IRQs (external + virtual) */
+#define IPIPE_NR_IRQS     (IPIPE_NR_XIRQS + IPIPE_NR_VIRQS)
+/* Number of indirect words needed to map the whole IRQ space. */
+#define IPIPE_IRQ_IWORDS  ((IPIPE_NR_IRQS + 31) / 32)
+#define IPIPE_IRQ_IMASK   (BITS_PER_LONG - 1)
+#define IPIPE_IRQ_ISHIFT  5	/* 2^5 for 32bits arch. */
+
+#define IPIPE_IRQMASK_ANY   (~0UL)
+
+typedef struct adomain {
+
+    /* -- Section: offset-based references are made on these fields
+       from inline assembly code. Please don't move or reorder. */
+    void (*dswitch)(void);	/* Domain switch hook */
+    int *esp[ADEOS_NR_CPUS];	/* Domain stack pointers */
+    /* -- End of section. */
+
+    int *estackbase[ADEOS_NR_CPUS];
+
+    unsigned long flags;
+
+    unsigned domid;
+
+    const char *name;
+
+    int priority;
+
+    struct adcpudata {
+	volatile unsigned long status;
+	volatile unsigned long irq_pending_hi;
+	volatile unsigned long irq_pending_lo[IPIPE_IRQ_IWORDS];
+	volatile unsigned irq_hits[IPIPE_NR_IRQS];
+	adevinfo_t event_info;
+    } cpudata[ADEOS_NR_CPUS];
+
+    struct {
+	int (*acknowledge)(unsigned irq);
+	void (*handler)(unsigned irq);
+	volatile unsigned long control;
+    } irqs[IPIPE_NR_IRQS];
+
+    struct {
+	void (*handler)(adevinfo_t *evinfo);
+    } events[ADEOS_NR_EVENTS];
+
+    int ptd_keymax;
+    int ptd_keycount;
+    unsigned long ptd_keymap;
+    void (*ptd_setfun)(int, void *);
+    void *(*ptd_getfun)(int);
+
+    struct adomain *m_link;	/* Link in mutex sleep queue */
+
+    struct list_head p_link;	/* Link in pipeline */
+
+} adomain_t;
+
+typedef struct admutex {
+
+	spinlock_t lock;
+
+      	adomain_t *sleepq,      /* Pending domains queue */
+	          *owner;	/* Domain owning the mutex */
+
+#define ADEOS_MUTEX_UNLOCKED { SPIN_LOCK_UNLOCKED, NULL, NULL }
+
+} admutex_t;
+
+#define __clear_bit(nr,addr) clear_bit(nr,addr)
+
+/* The following macros must be used hw interrupts off. */
+
+#define __adeos_set_irq_bit(adp,cpuid,irq) \
+do { \
+    if (!__test_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control)) { \
+        __set_bit(irq & IPIPE_IRQ_IMASK,&(adp)->cpudata[cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT]); \
+        __set_bit(irq >> IPIPE_IRQ_ISHIFT,&(adp)->cpudata[cpuid].irq_pending_hi); \
+       } \
+} while(0)
+
+#define __adeos_clear_pend(adp,cpuid,irq) \
+do { \
+    __clear_bit(irq & IPIPE_IRQ_IMASK,&(adp)->cpudata[cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT]); \
+    if ((adp)->cpudata[cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT] == 0) \
+        __clear_bit(irq >> IPIPE_IRQ_ISHIFT,&(adp)->cpudata[cpuid].irq_pending_hi); \
+} while(0)
+
+#define __adeos_lock_irq(adp,cpuid,irq) \
+do { \
+    if (!__test_and_set_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control)) \
+	__adeos_clear_pend(adp,cpuid,irq); \
+} while(0)
+
+#define __adeos_unlock_irq(adp,irq) \
+do { \
+    int __cpuid, __nr_cpus = num_online_cpus();			       \
+    if (__test_and_clear_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control)) \
+	for (__cpuid = 0; __cpuid < __nr_cpus; __cpuid++)      \
+         if ((adp)->cpudata[__cpuid].irq_hits[irq] > 0) { \
+           __set_bit(irq & IPIPE_IRQ_IMASK,&(adp)->cpudata[__cpuid].irq_pending_lo[irq >> IPIPE_IRQ_ISHIFT]); \
+           __set_bit(irq >> IPIPE_IRQ_ISHIFT,&(adp)->cpudata[__cpuid].irq_pending_hi); \
+         } \
+} while(0)
+
+#define __adeos_clear_irq(adp,irq) \
+do { \
+    int __cpuid, __nr_cpus = num_online_cpus(); \
+    __clear_bit(IPIPE_LOCK_FLAG,&(adp)->irqs[irq].control); \
+    for (__cpuid = 0; __cpuid < __nr_cpus; __cpuid++) {	\
+       (adp)->cpudata[__cpuid].irq_hits[irq] = 0; \
+       __adeos_clear_pend(adp,__cpuid,irq); \
+    } \
+} while(0)
+
+#define adeos_virtual_irq_p(irq) ((irq) >= IPIPE_VIRQ_BASE && \
+				  (irq) < IPIPE_NR_IRQS)
+
+#define adeos_hw_test_iflag(x)        (((x) & PSR_I_BIT)) /* We don't use the FIRQs */
+
+#define adeos_hw_irqs_disabled()	\
+({					\
+	unsigned long flags;		\
+	adeos_hw_local_irq_flags(flags);	\
+	adeos_hw_test_iflag(flags);	\
+})
+
+#define adeos_hw_tsc(t)  0
+#define adeos_cpu_freq() CLOCK_TICK_RATE
+
+#ifdef CONFIG_PREEMPT
+#define adeos_spin_lock(x)    _raw_spin_lock(x)
+#define adeos_spin_unlock(x)  _raw_spin_unlock(x)
+#define adeos_spin_trylock(x) _raw_spin_trylock(x)
+#else /* !CONFIG_PREEMPT */
+#define adeos_spin_lock(x)    spin_lock(x)
+#define adeos_spin_unlock(x)  spin_unlock(x)
+#define adeos_spin_trylock(x) spin_trylock(x)
+#endif /* CONFIG_PREEMPT */
+
+#define adeos_spin_lock_irqsave(x,flags)       do { adeos_hw_local_irq_save(flags); adeos_spin_lock(x); } while (0)
+#define adeos_spin_unlock_irqrestore(x,flags)  do { adeos_spin_unlock(x); adeos_hw_local_irq_restore(flags); } while (0)
+#define adeos_spin_lock_disable(x)             do { adeos_hw_cli(); adeos_spin_lock(x); } while (0)
+#define adeos_spin_unlock_enable(x)            do { adeos_spin_unlock(x); adeos_hw_sti(); } while (0)
+
+/* Private interface -- Internal use only */
+
+struct adattr;
+
+void __adeos_init(void);
+
+void __adeos_init_domain(adomain_t *adp,
+			 struct adattr *attr);
+
+void __adeos_cleanup_domain(adomain_t *adp);
+
+#define __adeos_check_machine() do { } while(0)
+
+void __adeos_enable_pipeline(void);
+
+void __adeos_disable_pipeline(void);
+
+void __adeos_init_stage(adomain_t *adp);
+
+void __adeos_sync_stage(void);
+
+void __adeos_tune_timer(unsigned long hz);
+
+void __adeos_set_timer (unsigned long hz);
+
+asmlinkage int __adeos_handle_irq(int irq,
+				  struct pt_regs *regs);
+
+void __adeos_macro_disable_irq (void);
+void  __adeos_macro_enable_irq (void);
+
+asmlinkage void __adeos_wrap_schedule(void);
+asmlinkage void __adeos_wrap_syscall_trace(int why, struct pt_regs *regs);
+asmlinkage int __adeos_wrap_do_notify_resume( struct pt_regs *regs, unsigned int thread_flags, int syscall);
+
+asmlinkage int __adeos_switch_domain(adomain_t *adp,
+				     adomain_t **currentp);
+
+static inline void __adeos_switch_to(adomain_t *adp, int cpuid) {
+
+    extern adomain_t *adp_cpu_current[];
+    unsigned long flags;
+
+    adeos_hw_local_irq_save(flags);
+    __adeos_switch_domain(adp,&adp_cpu_current[cpuid]);
+    adeos_hw_local_irq_restore(flags);
+
+    if (adp_cpu_current[adeos_processor_id()]->dswitch != NULL)
+	adp_cpu_current[adeos_processor_id()]->dswitch();
+}
+
+extern struct pt_regs __adeos_irq_regs;
+
+extern unsigned long __adeos_virtual_irq_map;
+
+extern int (*adeos_irq_entry)(int irq, struct pt_regs *regs);
+extern int (*adeos_syscall_entry)(struct pt_regs *regs);
+#endif /* !__ARM_ADEOS_H */
diff --git a/include/asm-arm/arch-imx/hardware.h b/include/asm-arm/arch-imx/hardware.h
index a672dc8..79149e6 100644
--- a/include/asm-arm/arch-imx/hardware.h
+++ b/include/asm-arm/arch-imx/hardware.h
@@ -26,9 +26,11 @@
 #ifndef __ASSEMBLY__
 # define __REG(x)	(*((volatile u32 *)IO_ADDRESS(x)))

-# define __REG2(x,y)	\
+/*# define __REG2(x,y)	\
 	( __builtin_constant_p(y) ? (__REG((x) + (y))) \
 			  : (*(volatile u32 *)((u32)&__REG(x) + (y))) )
+*/ /* with gcc-3.3.5 (at least) deprecated warning */
+# define __REG2(x,y)        (*(volatile u32 *)((u32)&__REG(x) + (y)))
 #endif

 /*
diff --git a/include/asm-arm/arch-imx/io.h b/include/asm-arm/arch-imx/io.h
index e9e29b6..90a2968 100644
--- a/include/asm-arm/arch-imx/io.h
+++ b/include/asm-arm/arch-imx/io.h
@@ -23,6 +23,8 @@
 #define IO_SPACE_LIMIT 0xffffffff

 #define __io(a)			(a)
+#define __mem_pci(a)	(a)
+

 /*
  * Validate the pci memory address for ioremap.
diff --git a/include/asm-arm/arch-imx/time.h b/include/asm-arm/arch-imx/time.h
index b042146..6787eb2 100644
--- a/include/asm-arm/arch-imx/time.h
+++ b/include/asm-arm/arch-imx/time.h
@@ -58,6 +58,8 @@ typedef struct TimerStruct {

 extern unsigned long (*gettimeoffset) (void);

+void timer_tick(struct pt_regs *regs);
+
 /*
  * Returns number of ms since last clock interrupt.  Note that interrupts
  * will have been disabled by do_gettimeoffset()
@@ -65,7 +67,7 @@ extern unsigned long (*gettimeoffset) (void);
 static unsigned long
 imx_gettimeoffset(void)
 {
-	volatile TimerStruct_t *timer1 = (TimerStruct_t *) TIMER1_VA_BASE;
+	volatile TimerStruct_t *timer1 = (TimerStruct_t *) TIMER0_VA_BASE;
 	unsigned long ticks;

 	/*
@@ -91,11 +93,11 @@ imx_gettimeoffset(void)
 /*
  * IRQ handler for the timer
  */
-static irqreturn_t
+irqreturn_t
 imx_timer_interrupt(int irq, void *dev_id, struct pt_regs *regs)
 {
 	volatile TimerStruct_t *timer1 =
-	    (volatile TimerStruct_t *) TIMER1_VA_BASE;
+	    (volatile TimerStruct_t *) TIMER0_VA_BASE;
 	// ...clear the interrupt
 	if (timer1->TimerClear) {
 		timer1->TimerClear = 0x0;
@@ -127,13 +129,12 @@ time_init(void)
 	timer1->TimerControl = 0;
 	timer0->TimerPrescaler = 0;
 	timer1->TimerPrescaler = 0;
-
-	timer1->TimerCompare = 328;
-	timer1->TimerControl = (TIM_32KHZ | TIM_INTEN | TIM_ENAB);
+	timer0->TimerCompare = 328;
+	timer0->TimerControl = (TIM_32KHZ | TIM_INTEN | TIM_ENAB);

 	/*
 	 * Make irqs happen for the system timer
 	 */
-	setup_irq(TIM2_INT, &timer_irq);
+	setup_irq(TIM1_INT, &timer_irq);
 	gettimeoffset = imx_gettimeoffset;
 }
diff --git a/include/asm-arm/arch-imx/timex.h b/include/asm-arm/arch-imx/timex.h
index 82b19e8..fb57ad0 100644
--- a/include/asm-arm/arch-imx/timex.h
+++ b/include/asm-arm/arch-imx/timex.h
@@ -21,4 +21,4 @@
 /*
  * ??
  */
-#define CLOCK_TICK_RATE		(50000000 / 16)
+#define CLOCK_TICK_RATE		32768
diff --git a/include/asm-arm/atomic.h b/include/asm-arm/atomic.h
index 257d32f..48ada2c 100644
--- a/include/asm-arm/atomic.h
+++ b/include/asm-arm/atomic.h
@@ -17,7 +17,7 @@ typedef struct { volatile int counter; } atomic_t;

 #define ATOMIC_INIT(i)	{ (i) }

-#ifdef __KERNEL__
+//#ifdef __KERNEL__

 #define atomic_read(v)	((v)->counter)

@@ -139,9 +139,17 @@ static inline void atomic_add(int i, atomic_t *v)
 {
 	unsigned long flags;

+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_save(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_save(flags);
+#endif /* CONFIG_ADEOS_CORE */
 	v->counter += i;
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_restore(flags);
+#endif /* CONFIG_ADEOS_CORE */
 }

 static inline int atomic_add_return(int i, atomic_t *v)
@@ -149,10 +157,18 @@ static inline int atomic_add_return(int i, atomic_t *v)
 	unsigned long flags;
 	int val;

+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_save(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_save(flags);
+#endif /* CONFIG_ADEOS_CORE */
 	val = v->counter;
 	v->counter = val += i;
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_restore(flags);
+#endif /* CONFIG_ADEOS_CORE */

 	return val;
 }
@@ -161,9 +177,17 @@ static inline void atomic_sub(int i, atomic_t *v)
 {
 	unsigned long flags;

+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_save(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_save(flags);
+#endif /* CONFIG_ADEOS_CORE */
 	v->counter -= i;
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_restore(flags);
+#endif /* CONFIG_ADEOS_CORE */
 }

 static inline int atomic_sub_return(int i, atomic_t *v)
@@ -171,10 +195,18 @@ static inline int atomic_sub_return(int i, atomic_t *v)
 	unsigned long flags;
 	int val;

+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_save(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_save(flags);
+#endif /* CONFIG_ADEOS_CORE */
 	val = v->counter;
 	v->counter = val -= i;
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_restore(flags);
+#endif /* CONFIG_ADEOS_CORE */

 	return val;
 }
@@ -183,9 +215,17 @@ static inline void atomic_clear_mask(unsigned long mask, unsigned long *addr)
 {
 	unsigned long flags;

+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_save(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_save(flags);
+#endif /* CONFIG_ADEOS_CORE */
 	*addr &= ~mask;
+#ifdef CONFIG_ADEOS_CORE
+	adeos_hw_local_irq_restore(flags);
+#else /* !CONFIG_ADEOS_CORE */
 	local_irq_restore(flags);
+#endif /* CONFIG_ADEOS_CORE */
 }

 #endif /* __LINUX_ARM_ARCH__ */
@@ -204,5 +244,5 @@ static inline void atomic_clear_mask(unsigned long mask, unsigned long *addr)
 #define smp_mb__before_atomic_inc()	barrier()
 #define smp_mb__after_atomic_inc()	barrier()

-#endif
+//#endif
 #endif
diff --git a/include/asm-arm/bitops.h b/include/asm-arm/bitops.h
index 7fb3046..4b698d7 100644
--- a/include/asm-arm/bitops.h
+++ b/include/asm-arm/bitops.h
@@ -17,7 +17,10 @@
 #ifndef __ASM_ARM_BITOPS_H
 #define __ASM_ARM_BITOPS_H

-#ifdef __KERNEL__
+//#ifdef __KERNEL__
+
+#define likely(x)	__builtin_expect(!!(x), 1)
+#define unlikely(x)	__builtin_expect(!!(x), 0)

 #include <asm/system.h>

@@ -36,9 +39,19 @@ static inline void ____atomic_set_bit(unsigned int bit, volatile unsigned long *

 	p += bit >> 5;

-	local_irq_save(flags);
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_save(flags);
+#else
+        local_irq_save(flags);
+#endif
+
 	*p |= mask;
-	local_irq_restore(flags);
+
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_restore(flags);
+#else
+        local_irq_restore(flags);
+#endif
 }

 static inline void ____atomic_clear_bit(unsigned int bit, volatile unsigned long *p)
@@ -48,9 +61,19 @@ static inline void ____atomic_clear_bit(unsigned int bit, volatile unsigned long

 	p += bit >> 5;

-	local_irq_save(flags);
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_save(flags);
+#else
+        local_irq_save(flags);
+#endif
+
 	*p &= ~mask;
-	local_irq_restore(flags);
+
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_restore(flags);
+#else
+        local_irq_restore(flags);
+#endif
 }

 static inline void ____atomic_change_bit(unsigned int bit, volatile unsigned long *p)
@@ -60,9 +83,19 @@ static inline void ____atomic_change_bit(unsigned int bit, volatile unsigned lon

 	p += bit >> 5;

-	local_irq_save(flags);
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_save(flags);
+#else
+        local_irq_save(flags);
+#endif
+
 	*p ^= mask;
-	local_irq_restore(flags);
+
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_restore(flags);
+#else
+        local_irq_restore(flags);
+#endif
 }

 static inline int
@@ -74,10 +107,20 @@ ____atomic_test_and_set_bit(unsigned int bit, volatile unsigned long *p)

 	p += bit >> 5;

-	local_irq_save(flags);
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_save(flags);
+#else
+        local_irq_save(flags);
+#endif
+
 	res = *p;
 	*p = res | mask;
-	local_irq_restore(flags);
+
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_restore(flags);
+#else
+        local_irq_restore(flags);
+#endif

 	return res & mask;
 }
@@ -91,10 +134,20 @@ ____atomic_test_and_clear_bit(unsigned int bit, volatile unsigned long *p)

 	p += bit >> 5;

-	local_irq_save(flags);
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_save(flags);
+#else
+        local_irq_save(flags);
+#endif
+
 	res = *p;
 	*p = res & ~mask;
-	local_irq_restore(flags);
+
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_restore(flags);
+#else
+        local_irq_restore(flags);
+#endif

 	return res & mask;
 }
@@ -108,10 +161,20 @@ ____atomic_test_and_change_bit(unsigned int bit, volatile unsigned long *p)

 	p += bit >> 5;

-	local_irq_save(flags);
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_save(flags);
+#else
+        local_irq_save(flags);
+#endif
+
 	res = *p;
 	*p = res ^ mask;
-	local_irq_restore(flags);
+
+#ifdef CONFIG_ADEOS_CORE
+        adeos_hw_local_irq_restore(flags);
+#else
+        local_irq_restore(flags);
+#endif

 	return res & mask;
 }
@@ -411,6 +474,6 @@ static inline int sched_find_first_bit(unsigned long *b)
 #define minix_find_first_zero_bit(p,sz)		\
 		_find_first_zero_bit_le(p,sz)

-#endif /* __KERNEL__ */
+//#endif /* __KERNEL__ */

 #endif /* _ARM_BITOPS_H */
diff --git a/include/asm-arm/io.h b/include/asm-arm/io.h
index 9d7b1be..56ccef8 100644
--- a/include/asm-arm/io.h
+++ b/include/asm-arm/io.h
@@ -166,8 +166,8 @@ extern void _memset_io(unsigned long, int, size_t);
 #define writesl(p,d,l)		__raw_writesl((unsigned int)__mem_pci(p),d,l)

 #define memset_io(c,v,l)	_memset_io(__mem_pci(c),(v),(l))
-#define memcpy_fromio(a,c,l)	_memcpy_fromio((a),__mem_pci(c),(l))
-#define memcpy_toio(c,a,l)	_memcpy_toio(__mem_pci(c),(a),(l))
+//#define memcpy_fromio(a,c,l)	_memcpy_fromio((a),__mem_pci(c),(l))
+//#define memcpy_toio(c,a,l)	_memcpy_toio(__mem_pci(c),(a),(l))

 #define eth_io_copy_and_sum(s,c,l,b) \
 				eth_copy_and_sum((s),__mem_pci(c),(l),(b))
@@ -190,7 +190,6 @@ out:
 }

 #elif !defined(readb)
-
 #define readb(c)			(__readwrite_bug("readb"),0)
 #define readw(c)			(__readwrite_bug("readw"),0)
 #define readl(c)			(__readwrite_bug("readl"),0)
diff --git a/include/asm-arm/smp.h b/include/asm-arm/smp.h
index 5ca7716..4ac819c 100644
--- a/include/asm-arm/smp.h
+++ b/include/asm-arm/smp.h
@@ -5,6 +5,8 @@

 #ifdef CONFIG_SMP
 #error SMP not supported
+#else
+#define smp_num_cpus		1
 #endif

 #endif
diff --git a/include/asm-arm/system.h b/include/asm-arm/system.h
index d500cc9..bb7e187 100644
--- a/include/asm-arm/system.h
+++ b/include/asm-arm/system.h
@@ -1,7 +1,7 @@
 #ifndef __ASM_ARM_SYSTEM_H
 #define __ASM_ARM_SYSTEM_H

-#ifdef __KERNEL__
+//#ifdef __KERNEL__

 #include <linux/config.h>

@@ -94,7 +94,9 @@ void hook_fault_code(int nr, int (*fn)(unsigned long, unsigned int,

 #define tas(ptr) (xchg((ptr),1))

+#ifdef __KERNEL__
 extern asmlinkage void __backtrace(void);
+#endif

 extern int cpu_architecture(void);

@@ -161,8 +163,12 @@ do {									\
  * spin_unlock_irq() and friends are implemented.  This avoids
  * us needlessly decrementing and incrementing the preempt count.
  */
+#ifndef CONFIG_ADEOS_CORE
 #define prepare_arch_switch(rq,next)	local_irq_enable()
 #define finish_arch_switch(rq,prev)	spin_unlock(&(rq)->lock)
+#else
+#define finish_arch_switch(rq, next)	spin_unlock_irq(&(rq)->lock)
+#endif   /*CONFIG_ADEOS_CORE*/
 #define task_running(rq,p)		((rq)->curr == (p))
 #endif

@@ -184,31 +190,30 @@ do {									\
  * CPU interrupt mask handling.
  */
 #if __LINUX_ARM_ARCH__ >= 6
-
-#define local_irq_save(x)					\
+#define hard_local_irq_save(x)					\
 	({							\
 	__asm__ __volatile__(					\
-	"mrs	%0, cpsr		@ local_irq_save\n"	\
+	"mrs	%0, cpsr		@ hard_local_irq_save\n"	\
 	"cpsid	i"						\
 	: "=r" (x) : : "memory", "cc");				\
 	})

-#define local_irq_enable()  __asm__("cpsie i	@ __sti" : : : "memory", "cc")
-#define local_irq_disable() __asm__("cpsid i	@ __cli" : : : "memory", "cc")
-#define local_fiq_enable()  __asm__("cpsie f	@ __stf" : : : "memory", "cc")
-#define local_fiq_disable() __asm__("cpsid f	@ __clf" : : : "memory", "cc")
+#define hard_local_irq_enable()  __asm__("cpsie i	@ __sti" : : : "memory", "cc")
+#define hard_local_irq_disable() __asm__("cpsid i	@ __cli" : : : "memory", "cc")
+#define hard_local_fiq_enable()  __asm__("cpsie f	@ __stf" : : : "memory", "cc")
+#define hard_local_fiq_disable() __asm__("cpsid f	@ __clf" : : : "memory", "cc")

 #else

 /*
  * Save the current interrupt enable state & disable IRQs
  */
-#define local_irq_save(x)					\
+#define hard_local_irq_save(x)					\
 	({							\
 		unsigned long temp;				\
 		(void) (&temp == &x);				\
 	__asm__ __volatile__(					\
-	"mrs	%0, cpsr		@ local_irq_save\n"	\
+	"mrs	%0, cpsr		@ hard_local_irq_save\n"	\
 "	orr	%1, %0, #128\n"					\
 "	msr	cpsr_c, %1"					\
 	: "=r" (x), "=r" (temp)					\
@@ -219,11 +224,11 @@ do {									\
 /*
  * Enable IRQs
  */
-#define local_irq_enable()					\
+#define hard_local_irq_enable()					\
 	({							\
 		unsigned long temp;				\
 	__asm__ __volatile__(					\
-	"mrs	%0, cpsr		@ local_irq_enable\n"	\
+	"mrs	%0, cpsr		@ hard_local_irq_enable\n"	\
 "	bic	%0, %0, #128\n"					\
 "	msr	cpsr_c, %0"					\
 	: "=r" (temp)						\
@@ -234,11 +239,12 @@ do {									\
 /*
  * Disable IRQs
  */
-#define local_irq_disable()					\
+
+#define hard_local_irq_disable()					\
 	({							\
 		unsigned long temp;				\
 	__asm__ __volatile__(					\
-	"mrs	%0, cpsr		@ local_irq_disable\n"	\
+	"mrs	%0, cpsr		@ hard_local_irq_disable\n"	\
 "	orr	%0, %0, #128\n"					\
 "	msr	cpsr_c, %0"					\
 	: "=r" (temp)						\
@@ -249,11 +255,12 @@ do {									\
 /*
  * Enable FIQs
  */
-#define __stf()							\
+
+#define hard__stf()							\
 	({							\
 		unsigned long temp;				\
 	__asm__ __volatile__(					\
-	"mrs	%0, cpsr		@ stf\n"		\
+	"mrs	%0, cpsr		@ hard_stf\n"		\
 "	bic	%0, %0, #64\n"					\
 "	msr	cpsr_c, %0"					\
 	: "=r" (temp)						\
@@ -264,41 +271,42 @@ do {									\
 /*
  * Disable FIQs
  */
-#define __clf()							\
+#define hard__clf()							\
 	({							\
 		unsigned long temp;				\
 	__asm__ __volatile__(					\
-	"mrs	%0, cpsr		@ clf\n"		\
+	"mrs	%0, cpsr		@ hard_clf\n"		\
 "	orr	%0, %0, #64\n"					\
 "	msr	cpsr_c, %0"					\
 	: "=r" (temp)						\
 	:							\
 	: "memory", "cc");					\
 	})
-
 #endif

 /*
  * Save the current interrupt enable state.
  */
-#define local_save_flags(x)					\
+
+#define hard_local_save_flags(x)					\
 	({							\
 	__asm__ __volatile__(					\
-	"mrs	%0, cpsr		@ local_save_flags"	\
+	"mrs	%0, cpsr		@ hard_local_save_flags"	\
 	: "=r" (x) : : "memory", "cc");				\
 	})

 /*
  * restore saved IRQ & FIQ state
  */
-#define local_irq_restore(x)					\
+
+#define hard_local_irq_restore(x)					\
 	__asm__ __volatile__(					\
 	"msr	cpsr_c, %0		@ local_irq_restore\n"	\
 	:							\
 	: "r" (x)						\
 	: "memory", "cc")

-#ifdef CONFIG_SMP
+#ifdef CONFIG_SMP		/*SMP not available for ARM*/
 #error SMP not supported

 #define smp_mb()		mb()
@@ -313,16 +321,72 @@ do {									\
 #define smp_wmb()		barrier()
 #define smp_read_barrier_depends()		do { } while(0)

-#define clf()			__clf()
-#define stf()			__stf()
+#define clf()			hard__clf()
+#define stf()			hard__stf()

-#define irqs_disabled()			\
+
+#define hard_irqs_disabled()			\
 ({					\
 	unsigned long flags;		\
-	local_save_flags(flags);	\
+	hard_local_save_flags(flags);	\
 	flags & PSR_I_BIT;		\
 })

+#ifdef CONFIG_ADEOS_CORE
+
+extern void __adeos_stall_root(void);
+
+extern void __adeos_unstall_root(void);
+
+extern unsigned long __adeos_test_root(void);
+
+extern unsigned long __adeos_test_and_stall_root(void);
+
+void __adeos_restore_root(unsigned long flags);
+
+void __adeos_restore_root_nosync (unsigned long flags);
+
+#define __stf()                 __adeos_unstall_root()
+#define __clf()                 __adeos_stall_root()
+
+#define local_save_flags(x)	((x) = __adeos_test_root())
+#define local_irq_save(x)	((x) = __adeos_test_and_stall_root())
+#define local_irq_restore(x)	__adeos_restore_root(x)
+#define local_irq_disable()	__adeos_stall_root()
+#define local_irq_enable()	__adeos_unstall_root()
+
+#define irqs_disabled()         __adeos_test_root()
+
+#define adeos_hw_cli()   	hard_local_irq_disable()
+
+#define adeos_hw_sti()		hard_local_irq_enable()
+
+#define adeos_hw_stf()		hard__stf()
+
+#define adeos_hw_clf()		hard__clf()
+
+#define adeos_hw_local_irq_save(x) 		hard_local_irq_save(x)
+
+#define adeos_hw_local_irq_restore(x) 		hard_local_irq_restore(x)
+
+#define adeos_hw_local_irq_flags(x) 		hard_local_save_flags(x)
+
+#else /* !CONFIG_ADEOS_CORE */
+
+#define local_irq_save(x)		hard_local_irq_save(x)
+#define local_irq_enable()		hard_local_irq_enable()
+#define local_irq_disable()		hard_local_irq_disable()
+#define local_fiq_enable()		hard_local_fiq_enable()
+#define local_fiq_disable()		hard_local_fiq_disable()
+#define __stf()					hard__stf()
+#define __clf()					hard__clf()
+#define local_save_flags(x)		hard_local_save_flags(x)
+#define local_irq_restore(x)		hard_local_irq_restore(x)
+#define irqs_disabled()			hard_irqs_disabled()
+#endif /* CONFIG_ADEOS_CORE */
+
+#endif
+
 #if defined(CONFIG_CPU_SA1100) || defined(CONFIG_CPU_SA110)
 /*
  * On the StrongARM, "swp" is terminally broken since it bypasses the
@@ -384,6 +448,5 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size

 #endif /* __ASSEMBLY__ */

-#endif /* __KERNEL__ */
+//#endif /* __KERNEL__ */

-#endif
diff --git a/include/linux/adeos.h b/include/linux/adeos.h
new file mode 100644
index 0000000..b3ce3a5
--- /dev/null
+++ b/include/linux/adeos.h
@@ -0,0 +1,396 @@
+/*
+ *   include/linux/adeos.h
+ *
+ *   Copyright (C) 2002,2003,2004 Philippe Gerum.
+ *   RTAI/ARM over Adeos rewrite for PXA255_2.6.7:
+ *   Copyright (c) 2005 Stefano Gafforelli (stefano.gafforelli@tiscali.it)
+ *   Copyright (c) 2005 Luca Pizzi (lucapizzi@hotmail.com)
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+
+#ifndef __LINUX_ADEOS_H
+#define __LINUX_ADEOS_H
+
+#include <linux/kernel.h>
+
+/* be paranoid and check assumptions (mostly if hw-irqs are off)? */
+#undef ADEOS_PARANOIA
+//#define ADEOS_PARANOIA
+#ifdef ADEOS_PARANOIA
+#define ADEOS_PARANOIA_ASSERT(cond)	BUG_ON(!(cond))
+#else
+#define ADEOS_PARANOIA_ASSERT(cond)	do { /* nop */ } while (0)
+#endif
+
+#include <asm-arm/bitops.h>
+#include <asm/adeos.h>
+
+#define ADEOS_VERSION_PREFIX  "2.6"
+#define ADEOS_VERSION_STRING  (ADEOS_VERSION_PREFIX ADEOS_ARCH_STRING)
+#define ADEOS_RELEASE_NUMBER  0x02040cff
+
+#define ADEOS_ROOT_PRI       100
+#define ADEOS_ROOT_ID        0
+#define ADEOS_ROOT_NPTDKEYS  4	/* Must be <= 32 */
+
+#define ADEOS_OTHER_CPUS   (-1)
+#define ADEOS_RESET_TIMER  0x1
+#define ADEOS_SAME_HANDLER ((void (*)(unsigned))(-1))
+
+/* Global domain flags */
+#define ADEOS_SPRINTK_FLAG 0	/* Synchronous printk() allowed */
+#define ADEOS_PPRINTK_FLAG 1	/* Asynchronous printk() request pending */
+
+#define IPIPE_STALL_FLAG   0	/* Stalls a pipeline stage */
+#define IPIPE_SYNC_FLAG    1	/* IRQ sync is undergoing */
+#define IPIPE_XPEND_FLAG   2	/* Exception notification is pending */
+#define IPIPE_SLEEP_FLAG   3	/* Domain has suspended itself */
+
+#define IPIPE_HANDLE_FLAG    0
+#define IPIPE_PASS_FLAG      1
+#define IPIPE_CALLASM_FLAG   2
+#define IPIPE_ENABLE_FLAG    3
+#define IPIPE_DYNAMIC_FLAG   IPIPE_HANDLE_FLAG
+#define IPIPE_EXCLUSIVE_FLAG 4
+#define IPIPE_STICKY_FLAG    5
+#define IPIPE_SYSTEM_FLAG    6
+#define IPIPE_LOCK_FLAG      7
+#define IPIPE_SHARED_FLAG    8
+
+#define IPIPE_HANDLE_MASK    (1 << IPIPE_HANDLE_FLAG)
+#define IPIPE_PASS_MASK      (1 << IPIPE_PASS_FLAG)
+#define IPIPE_CALLASM_MASK   (1 << IPIPE_CALLASM_FLAG)
+#define IPIPE_ENABLE_MASK    (1 << IPIPE_ENABLE_FLAG)
+#define IPIPE_DYNAMIC_MASK   IPIPE_HANDLE_MASK
+#define IPIPE_EXCLUSIVE_MASK (1 << IPIPE_EXCLUSIVE_FLAG)
+#define IPIPE_STICKY_MASK    (1 << IPIPE_STICKY_FLAG)
+#define IPIPE_SYSTEM_MASK    (1 << IPIPE_SYSTEM_FLAG)
+#define IPIPE_LOCK_MASK      (1 << IPIPE_LOCK_FLAG)
+#define IPIPE_SHARED_MASK    (1 << IPIPE_SHARED_FLAG)
+
+#define IPIPE_DEFAULT_MASK  (IPIPE_HANDLE_MASK|IPIPE_PASS_MASK)
+
+typedef struct adattr {
+
+    unsigned domid;		/* Domain identifier -- Magic value set by caller */
+    const char *name;		/* Domain name -- Warning: won't be dup'ed! */
+    int priority;		/* Priority in interrupt pipeline */
+    void (*entry)(int);		/* Domain entry point */
+    int estacksz;		/* Stack size for entry context -- 0 means unspec */
+    void (*dswitch)(void);	/* Handler called each time the domain is switched in */
+    int nptdkeys;		/* Max. number of per-thread data keys */
+    void (*ptdset)(int,void *);	/* Routine to set pt values */
+    void *(*ptdget)(int);	/* Routine to get pt values */
+
+} adattr_t;
+
+extern adomain_t *adp_cpu_current[],
+                 *adp_root;
+
+extern int __adeos_event_monitors[];
+
+extern unsigned __adeos_printk_virq;
+
+extern struct  irqchip __adeos_std_irq_chip[];
+
+/* Private interface */
+
+#ifdef CONFIG_PROC_FS
+void __adeos_init_proc(void);
+#endif /* CONFIG_PROC_FS */
+
+void __adeos_takeover(void);
+
+int __adeos_handle_event(unsigned event,
+		 void *evdata);
+
+void __adeos_sync_console(unsigned irq);
+
+void __adeos_dump_state(void);
+
+static inline void __adeos_schedule_head(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SCHEDULE_HEAD] > 0))
+	__adeos_handle_event(ADEOS_SCHEDULE_HEAD,evdata);
+}
+
+static inline int __adeos_schedule_tail(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SCHEDULE_TAIL] > 0))
+	return __adeos_handle_event(ADEOS_SCHEDULE_TAIL,evdata);
+
+    return 0;
+}
+
+static inline void __adeos_enter_process(void) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_ENTER_PROCESS] > 0))
+	__adeos_handle_event(ADEOS_ENTER_PROCESS,NULL);
+}
+
+static inline void __adeos_exit_process(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_EXIT_PROCESS] > 0))
+	__adeos_handle_event(ADEOS_EXIT_PROCESS,evdata);
+}
+
+static inline int __adeos_signal_process(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_SIGNAL_PROCESS] > 0))
+	return __adeos_handle_event(ADEOS_SIGNAL_PROCESS,evdata);
+
+    return 0;
+}
+
+static inline void __adeos_kick_process(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_KICK_PROCESS] > 0))
+	__adeos_handle_event(ADEOS_KICK_PROCESS,evdata);
+}
+
+static inline int __adeos_renice_process(void *evdata) {
+
+    if (unlikely(__adeos_event_monitors[ADEOS_RENICE_PROCESS] > 0))
+	return __adeos_handle_event(ADEOS_RENICE_PROCESS,evdata);
+
+    return 0;
+}
+void __adeos_stall_root (void);
+
+void __adeos_unstall_root (void);
+
+unsigned long __adeos_test_root (void);
+
+unsigned long __adeos_test_and_stall_root (void);
+
+//void FASTCALL(__adeos_restore_root(unsigned long flags));
+void (__adeos_restore_root(unsigned long flags));
+
+void __adeos_schedule_back_root(struct task_struct *prev);
+
+/* Public interface */
+
+int adeos_register_domain(adomain_t *adp,
+			  adattr_t *attr);
+
+int adeos_unregister_domain(adomain_t *adp);
+
+void adeos_renice_domain(int newpri);
+
+void adeos_suspend_domain(void);
+
+int adeos_virtualize_irq_from(adomain_t *adp,
+			      unsigned irq,
+			      void (*handler)(unsigned irq),
+			      int (*acknowledge)(unsigned irq),
+			      unsigned modemask);
+
+static inline int adeos_virtualize_irq(unsigned irq,
+				       void (*handler)(unsigned irq),
+				       int (*acknowledge)(unsigned irq),
+				       unsigned modemask) {
+
+    return adeos_virtualize_irq_from(adp_current,
+				     irq,
+				     handler,
+				     acknowledge,
+				     modemask);
+}
+
+int adeos_control_irq(unsigned irq,
+		      unsigned clrmask,
+		      unsigned setmask);
+
+unsigned long adeos_set_irq_affinity(unsigned irq,
+			     unsigned long cpumask);
+static inline int adeos_share_irq (unsigned irq, int (*acknowledge)(unsigned irq)) {
+
+    return adeos_virtualize_irq(irq,
+				ADEOS_SAME_HANDLER,
+				acknowledge,
+				IPIPE_SHARED_MASK|IPIPE_HANDLE_MASK|IPIPE_PASS_MASK);
+}
+
+unsigned adeos_alloc_irq(void);
+
+int adeos_free_irq(unsigned irq);
+
+int FASTCALL(adeos_trigger_irq(unsigned irq));
+
+int FASTCALL(adeos_propagate_irq(unsigned irq));
+
+int FASTCALL(adeos_schedule_irq(unsigned irq));
+
+int FASTCALL(adeos_trigger_ipi(int cpuid));
+
+static inline void adeos_stall_pipeline_from (adomain_t *adp) {
+
+    adeos_declare_cpuid;
+    unsigned long flags;
+
+    adeos_get_cpu(flags);
+    set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    adeos_put_cpu(flags);
+}
+
+static inline void adeos_stall_pipeline (void) {
+
+    adeos_stall_pipeline_from(adp_current);
+}
+
+void FASTCALL(adeos_unstall_pipeline_from(adomain_t *adp));
+
+static inline void adeos_unstall_pipeline (void) {
+
+    adeos_declare_cpuid;
+    unsigned long flags;
+
+    adeos_lock_cpu(flags);
+
+    __clear_bit(IPIPE_STALL_FLAG,&adp_current->cpudata[cpuid].status);
+
+    if (unlikely(adp_current->cpudata[cpuid].irq_pending_hi != 0))
+	__adeos_sync_stage();
+
+    adeos_unlock_cpu(flags);
+}
+
+static inline unsigned long adeos_test_pipeline_from (adomain_t *adp) {
+
+    unsigned long flags, s;
+    adeos_declare_cpuid;
+
+    adeos_get_cpu(flags);
+    s = test_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    adeos_put_cpu(flags);
+
+    return s;
+}
+
+static inline unsigned long adeos_test_pipeline (void) {
+
+    return adeos_test_pipeline_from(adp_current);
+}
+
+static inline unsigned long adeos_test_and_stall_pipeline_from (adomain_t *adp) {
+
+    unsigned long flags, s;
+    adeos_declare_cpuid;
+
+    adeos_get_cpu(flags);
+    s = test_and_set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    adeos_put_cpu(flags);
+
+    return s;
+}
+
+static inline unsigned long adeos_test_and_stall_pipeline (void) {
+
+    return adeos_test_and_stall_pipeline_from(adp_current);
+}
+
+static inline void adeos_restore_pipeline_from (adomain_t *adp, unsigned long flags) {
+
+    if (flags)
+	adeos_stall_pipeline_from(adp);
+    else
+	adeos_unstall_pipeline_from(adp);
+}
+
+static inline void adeos_restore_pipeline (unsigned long flags) {
+
+    unsigned long hwflags;
+    adeos_declare_cpuid;
+    adomain_t *adp;
+
+    adeos_lock_cpu(hwflags);
+
+    adp = adp_cpu_current[cpuid];
+
+    if (flags)
+	set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    else
+	{
+	__clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+
+	if (unlikely(adp->cpudata[cpuid].irq_pending_hi != 0))
+	    __adeos_sync_stage();
+	}
+
+    adeos_unlock_cpu(hwflags);
+}
+
+static inline void adeos_restore_pipeline_nosync (adomain_t *adp, unsigned long flags, int cpuid) {
+
+  /* If cpuid is current, then it must be held on entry
+      (adeos_get_cpu/adeos_hw_local_irq_save/adeos_hw_cli). */
+
+	unsigned long hwflags;
+    if (flags)
+	set_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+    else{
+		adeos_lock_cpu(hwflags);
+		__clear_bit(IPIPE_STALL_FLAG,&adp->cpudata[cpuid].status);
+		adeos_unlock_cpu(hwflags);
+	}
+}
+
+int adeos_catch_event_from(adomain_t *adp,
+			   unsigned event,
+			   void (*handler)(adevinfo_t *));
+
+static inline int adeos_catch_event (unsigned event, void (*handler)(adevinfo_t *)) {
+    return adeos_catch_event_from(adp_current,event,handler);
+}
+
+static inline void adeos_propagate_event(adevinfo_t *evinfo) {
+
+    evinfo->propagate = 1;
+}
+
+void (*adeos_hook_dswitch(void (*handler)(void)))(void);
+
+void adeos_init_attr(adattr_t *attr);
+
+int adeos_get_sysinfo(adsysinfo_t *sysinfo);
+
+int adeos_tune_timer(unsigned long ns,
+		     int flags);
+
+int adeos_alloc_ptdkey(void);
+
+int adeos_free_ptdkey(int key);
+
+int adeos_set_ptd(int key,
+		  void *value);
+
+void *adeos_get_ptd(int key);
+
+unsigned long adeos_critical_enter(void (*syncfn)(void));
+
+void adeos_critical_exit(unsigned long flags);
+
+int adeos_init_mutex(admutex_t *mutex);
+
+int adeos_destroy_mutex(admutex_t *mutex);
+
+unsigned long FASTCALL(adeos_lock_mutex(admutex_t *mutex));
+
+void FASTCALL(adeos_unlock_mutex(admutex_t *mutex,
+				 unsigned long flags));
+
+#endif /* !__LINUX_ADEOS_H */
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 5bc740d..ff02ba8 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -66,10 +66,9 @@ typedef struct irq_desc {
 	unsigned int irqs_unhandled;
 	spinlock_t lock;
 } ____cacheline_aligned irq_desc_t;
+//extern irq_desc_t irq_desc [NR_IRQS];

-extern irq_desc_t irq_desc [NR_IRQS];
-
-#include <asm/hw_irq.h> /* the arch dependent stuff */
+#include <asm/hardirq.h> /* the arch dependent stuff */

 extern int setup_irq(unsigned int , struct irqaction * );

diff --git a/include/linux/preempt.h b/include/linux/preempt.h
index a7ad901..317ab6d 100644
--- a/include/linux/preempt.h
+++ b/include/linux/preempt.h
@@ -25,12 +25,55 @@ do { \

 asmlinkage void preempt_schedule(void);

+#ifdef CONFIG_ADEOS_CORE
+
+#include <asm/adeos.h>
+
+extern adomain_t *adp_cpu_current[],
+                 *adp_root;
+
+#define preempt_disable() \
+do { \
+	if (adp_current == adp_root) { \
+   	    inc_preempt_count();       \
+	    barrier(); \
+        } \
+} while (0)
+
+#define preempt_enable_no_resched() \
+do { \
+        if (adp_current == adp_root) { \
+	    dec_preempt_count(); \
+	    barrier(); \
+        } \
+} while (0)
+
+#define preempt_check_resched() \
+do { \
+        if (adp_current == adp_root) { \
+	    if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
+		preempt_schedule(); \
+        } \
+} while (0)
+
+#define preempt_enable() \
+do { \
+	if (adp_current == adp_root) { \
+	    preempt_enable_no_resched(); \
+	    preempt_check_resched(); \
+        } \
+} while (0)
+
+#else /* !CONFIG_ADEOS_CORE */
+
 #define preempt_disable() \
 do { \
 	inc_preempt_count(); \
 	barrier(); \
 } while (0)

+#endif /* CONFIG_ADEOS_CORE */
+
 #define preempt_enable_no_resched() \
 do { \
 	barrier(); \
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d8d111..aaba383 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -135,6 +135,10 @@ struct sched_param {

 #include <linux/spinlock.h>

+#ifdef CONFIG_ADEOS_CORE
+#include <linux/adeos.h>
+#endif /* CONFIG_ADEOS_CORE */
+
 /*
  * This serializes "schedule()" and also protects
  * the run-queue from deletions/modifications (but
@@ -513,6 +517,10 @@ struct task_struct {
   	struct mempolicy *mempolicy;
   	short il_next;		/* could be shared with used_math */
 #endif
+
+#ifdef CONFIG_ADEOS_CORE
+	void *ptd[ADEOS_ROOT_NPTDKEYS]; // I moved here
+#endif
 };

 static inline pid_t process_group(struct task_struct *tsk)
diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 570778d..c4c03ad 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -35,6 +35,8 @@ extern void vunmap(void *addr);
  *	Lowlevel-APIs (not for driver use!)
  */
 extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
+extern struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,
+					unsigned long start, unsigned long end);
 extern struct vm_struct *remove_vm_area(void *addr);
 extern int map_vm_area(struct vm_struct *area, pgprot_t prot,
 			struct page ***pages);
diff --git a/init/main.c b/init/main.c
index 613aaab..b5a5647 100644
--- a/init/main.c
+++ b/init/main.c
@@ -427,10 +427,20 @@ asmlinkage void __init start_kernel(void)
 	parse_args("Booting kernel", command_line, __start___param,
 		   __stop___param - __start___param,
 		   &unknown_bootoption);
+
+	//evk9328_init_leds();
+	//scb9328_ledon(15);
+
 	sort_main_extable();
 	trap_init();
 	rcu_init();
 	init_IRQ();
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_init();
+#endif /*CONFIG_ADEOS_CORE*/
+
+	//scb9328_ledoff(8);
+
 	pidhash_init();
 	init_timers();
 	softirq_init();
@@ -445,7 +455,13 @@ asmlinkage void __init start_kernel(void)
 	if (panic_later)
 		panic(panic_later, panic_param);
 	profile_init();
+
+	//scb9328_ledoff(2);
+
 	local_irq_enable();
+
+	//scb9328_ledoff(1);
+
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (initrd_start && !initrd_below_start_ok &&
 			initrd_start < min_low_pfn << PAGE_SHIFT) {
@@ -628,6 +644,10 @@ static int init(void * unused)

 	do_basic_setup();

+#ifdef CONFIG_ADEOS
+//     __adeos_takeover();
+#endif /*CONFIG_ADEOS*/
+
 	/*
 	 * check if there is an early userspace init.  If yes, let it do all
 	 * the work
diff --git a/kernel/Makefile b/kernel/Makefile
index 238c65f..5297b0d 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -9,6 +9,7 @@ obj-y     = sched.o fork.o exec_domain.o panic.o printk.o profile.o \
 	    rcupdate.o intermodule.o extable.o params.o posix-timers.o \
 	    kthread.o

+obj-$(CONFIG_ADEOS_CORE) += adeos.o
 obj-$(CONFIG_FUTEX) += futex.o
 obj-$(CONFIG_GENERIC_ISA_DMA) += dma.o
 obj-$(CONFIG_SMP) += cpu.o
diff --git a/kernel/adeos.c b/kernel/adeos.c
new file mode 100644
index 0000000..f4184e6
--- /dev/null
+++ b/kernel/adeos.c
@@ -0,0 +1,579 @@
+
+/*
+ *   linux/kernel/adeos.c
+ *
+ *   Copyright (C) 2002,2003,2004 Philippe Gerum.
+ *   RTAI/ARM over Adeos rewrite for PXA255_2.6.7:
+ *   Copyright (c) 2005 Stefano Gafforelli (stefano.gafforelli@tiscali.it)
+ *   Copyright (c) 2005 Luca Pizzi (lucapizzi@hotmail.com)
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation, Inc., 675 Mass Ave, Cambridge MA 02139,
+ *   USA; either version 2 of the License, or (at your option) any later
+ *   version.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ *   Architecture-independent ADEOS core support.
+ */
+
+#include <linux/sched.h>
+#include <linux/module.h>
+#include <asm-arm/smp.h>
+#include <asm-arm/irq.h>
+#include <asm-arm/mach/irq.h>
+
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#endif /* CONFIG_PROC_FS */
+
+struct irqchip __adeos_std_irq_chip[NR_IRQS];
+EXPORT_SYMBOL(__adeos_std_irq_chip);
+
+/* The pre-defined domain slot for the root domain. */
+adomain_t adeos_root_domain;
+
+/* A constant pointer to the root domain. */
+adomain_t *adp_root = &adeos_root_domain;
+
+/* A pointer to the current domain. */
+adomain_t *adp_cpu_current[ADEOS_NR_CPUS] = { [ 0 ... ADEOS_NR_CPUS - 1] = &adeos_root_domain };
+
+/* The spinlock protecting from races while modifying the pipeline. */
+spinlock_t __adeos_pipelock = SPIN_LOCK_UNLOCKED;
+
+/* The pipeline data structure. Enqueues adomain_t objects by priority. */
+struct list_head __adeos_pipeline;
+
+/* An array of global counters tracking domains monitoring events. */
+int __adeos_event_monitors[ADEOS_NR_EVENTS] = { [ 0 ... ADEOS_NR_EVENTS - 1] = 0 };
+
+/* The allocated VIRQ map. */
+unsigned long __adeos_virtual_irq_map = 0;
+
+/* A VIRQ to kick printk() output out when the root domain is in control. */
+unsigned __adeos_printk_virq;
+
+static void __adeos_set_root_ptd (int key, void *value) {
+
+    current->ptd[key] = value;
+      // ptd[key] = value;
+}
+
+static void *__adeos_get_root_ptd (int key) {
+
+    return current->ptd[key];
+     // return ptd[key];
+}
+
+/* adeos_init() -- Initialization routine of the ADEOS layer. Called
+   by the host kernel early during the boot procedure. */
+
+void __adeos_init (void)
+
+{
+
+	adomain_t *adp = &adeos_root_domain;
+    __adeos_check_machine();	/* Do platform dependent checks first. */
+
+    /*
+      A lightweight registration code for the root domain. Current
+      assumptions are:
+      - We are running on the boot CPU, and secondary CPUs are still
+      lost in space.
+      - adeos_root_domain has been zero'ed.
+    */
+
+    INIT_LIST_HEAD(&__adeos_pipeline);
+
+    adp->name = "Linux";
+    adp->domid = ADEOS_ROOT_ID;
+    adp->priority = ADEOS_ROOT_PRI;
+    adp->ptd_setfun = &__adeos_set_root_ptd;
+    adp->ptd_getfun = &__adeos_get_root_ptd;
+    adp->ptd_keymax = ADEOS_ROOT_NPTDKEYS;
+
+    __adeos_init_stage(adp);
+
+    INIT_LIST_HEAD(&adp->p_link);
+    list_add_tail(&adp->p_link,&__adeos_pipeline);
+
+    __adeos_printk_virq = adeos_alloc_irq(); /* Cannot fail here. */
+    adp->irqs[__adeos_printk_virq].handler = &__adeos_sync_console;
+    adp->irqs[__adeos_printk_virq].acknowledge = NULL;
+    adp->irqs[__adeos_printk_virq].control = IPIPE_HANDLE_MASK;
+
+    printk(KERN_WARNING "Adeos %s: Root domain %s registered.\n",
+	   ADEOS_VERSION_STRING,
+	   adp->name);
+
+#ifndef CONFIG_ADEOS_MODULE
+    __adeos_takeover();
+#endif /* CONFIG_ADEOS_MODULE */
+
+}
+
+/* adeos_handle_event() -- Adeos' generic event handler. This routine
+calls the per-domain handlers registered for a given
+exception/event. Each domain before the one which raised the event in
+the pipeline will get a chance to process the event. The latter will
+eventually be allowed to process its own event too if a valid handler
+exists for it.  Handler executions are always scheduled by the domain
+which raised the event for the prioritary domains wanting to be
+notified of such event.  Note: evdata might be NULL. */
+
+asmlinkage int __adeos_handle_event (unsigned event, void *evdata)
+
+{
+    struct list_head *pos, *npos;
+    unsigned long flags;
+    adeos_declare_cpuid;
+    adevinfo_t evinfo;
+    int propagate = 1;
+
+    adeos_lock_cpu(flags);
+
+    list_for_each_safe(pos,npos,&__adeos_pipeline) {
+
+    	adomain_t *adp = list_entry(pos,adomain_t,p_link);
+
+	if (adp->events[event].handler != NULL)
+	    {
+	    if (adp == adp_cpu_current[cpuid])
+		{
+		evinfo.domid = adp->domid;
+		evinfo.event = event;
+		evinfo.evdata = evdata;
+		evinfo.propagate = 0;
+		adp->events[event].handler(&evinfo);
+		propagate = evinfo.propagate;
+		break;
+		}
+
+	    adp->cpudata[cpuid].event_info.domid = adp_cpu_current[cpuid]->domid;
+	    adp->cpudata[cpuid].event_info.event = event;
+	    adp->cpudata[cpuid].event_info.evdata = evdata;
+	    adp->cpudata[cpuid].event_info.propagate = 0;
+	    set_bit(IPIPE_XPEND_FLAG,&adp->cpudata[cpuid].status);
+
+	    /* Let the prioritary domain process the event. */
+	    __adeos_switch_to(adp,cpuid);
+
+#ifdef CONFIG_SMP
+	    adeos_load_cpuid();	/* Processor might have changed. */
+#endif /* CONFIG_SMP */
+
+	    if (!adp->cpudata[cpuid].event_info.propagate)
+		{
+		propagate = 0;
+		break;
+		}
+	    }
+
+	if (adp == adp_cpu_current[cpuid])
+	    break;
+    }
+
+    adeos_unlock_cpu(flags);
+
+    return !propagate;
+}
+
+void __adeos_stall_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	unsigned long flags;
+		adeos_hw_local_irq_save(flags);
+		__set_bit(IPIPE_STALL_FLAG, &adp_root->cpudata[adeos_processor_id()].status);
+		adeos_hw_local_irq_restore(flags);
+	}
+    else
+	adeos_hw_cli();
+}
+
+void __adeos_unstall_nosync_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	unsigned long flags;
+	adeos_hw_local_irq_save(flags);
+	__clear_bit(IPIPE_STALL_FLAG, &adp_root->cpudata[adeos_processor_id()].status);
+	adeos_hw_local_irq_restore(flags);
+	}
+}
+
+void __adeos_unstall_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	unsigned long hwflags;
+	int cpuid;
+	adeos_hw_local_irq_save(hwflags);
+	__clear_bit(IPIPE_STALL_FLAG, &(adp_root)->cpudata[cpuid = adeos_processor_id()].status);
+	if (adp_root->cpudata[cpuid].irq_pending_hi != 0)
+	    __adeos_sync_stage();
+	adeos_hw_local_irq_restore(hwflags);
+	}
+	adeos_hw_sti();
+}
+
+void __adeos_restore_root_nosync (unsigned long flags)
+{
+	if (flags) {
+		__adeos_stall_root();
+	} else {
+		__adeos_unstall_nosync_root ();
+	}
+}
+
+unsigned long __adeos_test_root (void)
+
+{
+    if (likely(adp_pipelined))
+	{
+	return __test_bit(IPIPE_STALL_FLAG, &adp_root->cpudata[adeos_processor_id()].status);
+	}
+
+    return adeos_hw_irqs_disabled();
+}
+
+unsigned long __adeos_test_and_stall_root (void)
+
+{
+    unsigned long flags;
+	adeos_hw_local_irq_save(flags);
+    if (likely(adp_pipelined))
+	{
+unsigned long s;
+	s = __test_and_set_bit(IPIPE_STALL_FLAG, &adp_root->cpudata[adeos_processor_id()].status);
+	adeos_hw_local_irq_restore(flags);
+	return s;
+	}
+
+    return adeos_hw_test_iflag(flags);
+}
+
+void __adeos_restore_root (unsigned long flags)
+{
+	if (flags)
+	    __adeos_stall_root();
+	else
+	    __adeos_unstall_root();
+}
+
+/* adeos_suspend_domain() -- tell the ADEOS layer that the current
+   domain is now dormant. The calling domain is switched out, while
+   the next domain with work in progress or pending in the pipeline is
+   switched in. */
+
+#define __flush_pipeline_stage() \
+do { \
+    if (!test_bit(IPIPE_STALL_FLAG,&cpudata->status) && \
+	cpudata->irq_pending_hi != 0) \
+	{ \
+	__adeos_sync_stage(); \
+	adeos_load_cpuid(); \
+	cpudata = &adp->cpudata[cpuid]; \
+	} \
+    if (test_and_clear_bit(IPIPE_XPEND_FLAG,&cpudata->status)) \
+	adp->events[cpudata->event_info.event].handler(&cpudata->event_info); \
+} while(0)
+
+void adeos_suspend_domain (void)
+
+{
+    struct adcpudata *cpudata;
+    adomain_t *adp, *nadp;
+    struct list_head *ln;
+    unsigned long flags;
+    adeos_declare_cpuid;
+
+    adeos_lock_cpu(flags);
+    adp = nadp = adp_cpu_current[cpuid];
+    cpudata = &adp->cpudata[cpuid];
+
+    /* A suspending domain implicitely unstalls the pipeline. */
+    __clear_bit(IPIPE_STALL_FLAG,&cpudata->status);
+
+    /* Make sure that no event remains stuck in the pipeline. This
+       could happen with emerging SMP instances, or domains which
+       forget to unstall their stage before calling us. */
+    __flush_pipeline_stage();
+
+
+    for (;;)
+	{
+	ln = nadp->p_link.next;
+
+	if (ln == &__adeos_pipeline)	/* End of pipeline reached? */
+	    /* Caller should loop on its idle task on return. */
+	    goto release_cpu_and_exit;
+
+	nadp = list_entry(ln,adomain_t,p_link);
+
+	/* Make sure the domain was preempted (i.e. not sleeping) or
+	   has some event to process before switching to it. */
+
+	if (!test_bit(IPIPE_SLEEP_FLAG,&nadp->cpudata[cpuid].status) ||
+	    (!test_bit(IPIPE_STALL_FLAG,&nadp->cpudata[cpuid].status) &&
+	     nadp->cpudata[cpuid].irq_pending_hi != 0) ||
+	    test_bit(IPIPE_XPEND_FLAG,&nadp->cpudata[cpuid].status))
+	    break;
+	}
+
+    /* Mark the outgoing domain as aslept (i.e. not preempted). */
+    set_bit(IPIPE_SLEEP_FLAG,&cpudata->status);
+
+    /* Suspend the calling domain, switching to the next one. */
+    __adeos_switch_to(nadp,cpuid);
+
+    /* Clear the sleep bit for the incoming domain (on the initial processor). */
+    __clear_bit(IPIPE_SLEEP_FLAG,&cpudata->status);
+
+#ifdef CONFIG_SMP
+    adeos_load_cpuid();	/* Processor might have changed. */
+    cpudata = &adp->cpudata[cpuid];
+#endif /* CONFIG_SMP */
+
+    /* Now, we are back into the calling domain. Flush the interrupt
+       log and fire the event interposition handler if needed.  CPU
+       migration is allowed in SMP-mode on behalf of an event handler
+       provided that the current domain raised it. Otherwise, it's
+       not. */
+
+    __flush_pipeline_stage();
+
+release_cpu_and_exit:
+
+    adeos_unlock_cpu(flags);
+
+    /* Return to the point of suspension in the calling domain. */
+}
+
+/* adeos_alloc_irq() -- Allocate a virtual/soft pipelined interrupt.
+  Virtual interrupts are handled in exactly the same way than their
+   hw-generated counterparts. This is a very basic, one-way only,
+   inter-domain communication system (see adeos_trigger_irq()).  Note:
+   it is not necessary for a domain to allocate a virtual interrupt to
+   trap it using adeos_virtualize_irq(). The newly allocated VIRQ
+   number which can be passed to other IRQ-related services is
+   returned on success, zero otherwise (i.e. no more virtual interrupt
+   channel is available). We need this service as part of the Adeos
+   bootstrap code, hence it must reside in a built-in area. */
+
+unsigned adeos_alloc_irq (void)
+
+{
+    unsigned long flags, irq = 0;
+    int ipos;
+
+    adeos_spin_lock_irqsave(&__adeos_pipelock,flags);
+
+    if (__adeos_virtual_irq_map != ~0)
+	{
+	ipos = ffz(__adeos_virtual_irq_map);
+	set_bit(ipos,&__adeos_virtual_irq_map);
+	irq = ipos + IPIPE_VIRQ_BASE;
+	}
+
+    adeos_spin_unlock_irqrestore(&__adeos_pipelock,flags);
+
+    return irq;
+}
+
+#ifdef CONFIG_PROC_FS
+
+#include <linux/proc_fs.h>
+
+static struct proc_dir_entry *adeos_proc_entry;
+
+static int __adeos_read_proc (char *page,
+			      char **start,
+			      off_t off,
+			      int count,
+			      int *eof,
+			      void *data)
+{
+    unsigned long flags, ctlbits;
+    struct list_head *pos;
+    unsigned irq, _irq;
+    char *p = page;
+    int len;
+
+#ifdef CONFIG_ADEOS_MODULE
+    p += sprintf(p,"Adeos %s -- Pipelining: %s\n\n",ADEOS_VERSION_STRING,adp_pipelined ? "active" : "stopped");
+#else /* !CONFIG_ADEOS_MODULE */
+    p += sprintf(p,"Adeos %s -- Pipelining: permanent\n\n",ADEOS_VERSION_STRING);
+#endif /* CONFIG_ADEOS_MODULE */
+
+    adeos_hw_local_irq_save(flags);
+
+    list_for_each(pos,&__adeos_pipeline) {
+
+    	adomain_t *adp = list_entry(pos,adomain_t,p_link);
+
+	p += sprintf(p,"%8s: priority=%d, id=0x%.8x, ptdkeys=%d/%d\n",
+		     adp->name,
+		     adp->priority,
+		     adp->domid,
+		     adp->ptd_keycount,
+		     adp->ptd_keymax);
+	irq = 0;
+
+	while (irq < IPIPE_NR_IRQS)
+	    {
+	    ctlbits = (adp->irqs[irq].control & (IPIPE_HANDLE_MASK|IPIPE_PASS_MASK|IPIPE_STICKY_MASK));
+
+	    if (adeos_virtual_irq_p(irq) && !test_bit(irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map))
+		{
+		irq++;
+		continue;
+		}
+	    /* Attempt to group consecutive IRQ numbers having the
+	       same virtualization settings in a single line. */
+	    _irq = irq;
+	    while (++_irq < IPIPE_NR_IRQS)
+		{
+		if (adeos_virtual_irq_p(_irq) != adeos_virtual_irq_p(irq) ||
+		    (adeos_virtual_irq_p(_irq) && !test_bit(_irq - IPIPE_VIRQ_BASE,&__adeos_virtual_irq_map)) ||
+		    ctlbits != (adp->irqs[_irq].control & (IPIPE_HANDLE_MASK|IPIPE_PASS_MASK|IPIPE_STICKY_MASK)))
+	    break;
+		}
+
+	    if (_irq == irq + 1)
+		p += sprintf(p,"\tirq%u: ",irq);
+	    else
+		p += sprintf(p,"\tirq%u-%u: ",irq,_irq - 1);
+
+	    /* Statuses are as follows:
+	       o "accepted" means handled _and_ passed down the
+	       pipeline.
+	       o "grabbed" means handled, but the interrupt might be
+	       terminated _or_ passed down the pipeline depending on
+	       what the domain handler asks for to Adeos.
+	       o "passed" means unhandled by the domain but passed
+	       down the pipeline.
+	       o "discarded" means unhandled and _not_ passed down the
+               pipeline. The interrupt merely disappears from the
+	       current domain down to the end of the pipeline. */
+
+	    if (ctlbits & IPIPE_HANDLE_MASK)
+		{
+		if (ctlbits & IPIPE_PASS_MASK)
+		    p += sprintf(p,"accepted");
+		else
+		    p += sprintf(p,"grabbed");
+		}
+	    else if (ctlbits & IPIPE_PASS_MASK)
+		p += sprintf(p,"passed");
+	    else
+		p += sprintf(p,"discarded");
+
+	    if (ctlbits & IPIPE_STICKY_MASK)
+		p += sprintf(p,", sticky");
+
+	    if (ctlbits & IPIPE_SHARED_MASK)
+		p += sprintf(p,", shared");
+
+	    if (adeos_virtual_irq_p(irq))
+		p += sprintf(p,", virtual");
+
+	    p += sprintf(p,"\n");
+
+	    irq = _irq;
+	    }
+    }
+
+    adeos_hw_local_irq_restore(flags);
+
+    len = p - page;
+
+    if (len <= off + count)
+	*eof = 1;
+
+    *start = page + off;
+
+    len -= off;
+    if (len > count)
+	len = count;
+
+    if (len < 0)
+	len = 0;
+
+    return len;
+}
+void __adeos_init_proc (void) {
+
+    adeos_proc_entry = create_proc_read_entry("adeos",
+				      0444,NULL,
+					      &__adeos_read_proc,
+					      NULL);
+}
+
+#endif /* CONFIG_PROC_FS */
+
+void __adeos_dump_state (void)
+
+{
+    int _cpuid,nr_cpus = smp_num_cpus;
+    struct list_head *pos;
+    unsigned long flags;
+    adeos_declare_cpuid;
+
+    adeos_lock_cpu(flags);
+
+    printk(KERN_WARNING "Adeos: Current domain=%s on CPU #%d [stackbase=%p]\n",
+	   adp_current->name,
+	   cpuid,
+	   (void *)adp_current->estackbase[cpuid]);
+
+    list_for_each(pos,&__adeos_pipeline) {
+
+        adomain_t *adp = list_entry(pos,adomain_t,p_link);
+
+        for (_cpuid = 0; _cpuid < nr_cpus; _cpuid++)
+            printk(KERN_WARNING "%8s[cpuid=%d]: priority=%d, status=0x%lx, pending_hi=0x%lx\n",
+                   adp->name,
+                   _cpuid,
+                   adp->priority,
+                   adp->cpudata[_cpuid].status,
+                   adp->cpudata[_cpuid].irq_pending_hi);
+    }
+
+    adeos_unlock_cpu(flags);
+}
+
+EXPORT_SYMBOL(__adeos_unstall_nosync_root);
+EXPORT_SYMBOL(adeos_suspend_domain);
+EXPORT_SYMBOL(adeos_alloc_irq);
+EXPORT_SYMBOL(adp_cpu_current);
+EXPORT_SYMBOL(adp_root);
+EXPORT_SYMBOL(__adeos_handle_event);
+EXPORT_SYMBOL(__adeos_unstall_root);
+EXPORT_SYMBOL(__adeos_stall_root);
+EXPORT_SYMBOL(__adeos_restore_root);
+EXPORT_SYMBOL(__adeos_restore_root_nosync);
+EXPORT_SYMBOL(__adeos_test_and_stall_root);
+EXPORT_SYMBOL(__adeos_test_root);
+EXPORT_SYMBOL(__adeos_dump_state);
+EXPORT_SYMBOL(__adeos_pipeline);
+EXPORT_SYMBOL(__adeos_pipelock);
+EXPORT_SYMBOL(__adeos_virtual_irq_map);
+EXPORT_SYMBOL(__adeos_event_monitors);
+/* The following are convenience exports which are needed by some
+   Adeos domains loaded as kernel modules. */
+EXPORT_SYMBOL(__mmdrop);
+EXPORT_SYMBOL(do_fork);
+EXPORT_SYMBOL(do_exit);
diff --git a/kernel/exit.c b/kernel/exit.c
index 3a51fec..9521334 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -793,6 +793,10 @@ asmlinkage NORET_TYPE void do_exit(long code)
 	}

 	acct_process(code);
+
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_exit_process(tsk);
+#endif /* CONFIG_ADEOS_CORE */
 	__exit_mm(tsk);

 	exit_sem(tsk);
diff --git a/kernel/fork.c b/kernel/fork.c
index 0cbc27f..2b79bc2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1095,6 +1095,16 @@ struct task_struct *copy_process(unsigned long clone_flags,

 	nr_threads++;
 	write_unlock_irq(&tasklist_lock);
+
+#ifdef CONFIG_ADEOS_CORE
+	{
+	int k;
+
+	for (k = 0; k < ADEOS_ROOT_NPTDKEYS; k++)
+	    p->ptd[k] = NULL;
+	}
+#endif /* CONFIG_ADEOS_CORE */
+
 	retval = 0;

 fork_out:
diff --git a/kernel/panic.c b/kernel/panic.c
index 3c1581e..7a31021 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -66,6 +66,11 @@ NORET_TYPE void panic(const char * fmt, ...)
 		printk(KERN_EMERG "In idle task - not syncing\n");
 	else
 		sys_sync();
+#ifdef CONFIG_ADEOS_CORE
+#ifndef CONFIG_ADEOS_MODULE
+	__adeos_dump_state();
+#endif
+#endif /* CONFIG_ADEOS_CORE */
 	bust_spinlocks(0);

 #ifdef CONFIG_SMP
diff --git a/kernel/printk.c b/kernel/printk.c
index 9e34ce4..ed3f1a5 100644
--- a/kernel/printk.c
+++ b/kernel/printk.c
@@ -33,6 +33,18 @@

 #include <asm/uaccess.h>

+#ifdef CONFIG_ADEOS_CORE
+#undef spin_lock_irq
+#define	spin_lock_irq(lock)                 adeos_spin_lock_disable(lock)
+#undef spin_unlock_irq
+#define	spin_unlock_irq(lock)               adeos_spin_unlock_enable(lock)
+#undef spin_lock_irqsave
+#define	spin_lock_irqsave(lock, flags)      adeos_spin_lock_irqsave(lock,flags)
+#undef spin_unlock_irqrestore
+#define spin_unlock_irqrestore(lock, flags) adeos_spin_unlock_irqrestore(lock,flags)
+#endif /* CONFIG_ADEOS_CORE */
+
+
 #define __LOG_BUF_LEN	(1 << CONFIG_LOG_BUF_SHIFT)

 /* printk's without a loglevel use this.. */
@@ -508,12 +520,15 @@ static void zap_locks(void)
 asmlinkage int printk(const char *fmt, ...)
 {
 	va_list args;
-	unsigned long flags;
+	unsigned long flags, swflag;
 	int printed_len;
 	char *p;
 	static char printk_buf[1024];
 	static int log_level_unknown = 1;

+#ifdef CONFIG_ADEOS_CORE
+	swflag = __adeos_test_root();
+#endif
 	if (unlikely(oops_in_progress))
 		zap_locks();

@@ -543,6 +558,24 @@ asmlinkage int printk(const char *fmt, ...)
 			log_level_unknown = 1;
 	}

+#ifdef CONFIG_ADEOS_CORE
+	if (adp_current != adp_root && !test_bit(ADEOS_SPRINTK_FLAG,&adp_current->flags)) {
+
+		/* When operating in asynchronous printk() mode, ensure the
+		 * console drivers and klogd wakeup are only run by Linux,
+		 * delegating the actual output to the root domain by mean of
+		 * a virtual IRQ kicking our sync handler. If the current
+		 * domain has a lower priority than Linux, then we'll get
+		 * immediately preempted by it. In synchronous printk() mode,
+		 * immediately call the console drivers.
+		 */
+		spin_unlock_irqrestore(&logbuf_lock, flags);
+		if (!test_and_set_bit(ADEOS_PPRINTK_FLAG,&adp_root->flags))
+			adeos_trigger_irq(__adeos_printk_virq);
+		goto out;
+	}
+#endif /* CONFIG_ADEOS_CORE */
+
 	if (!cpu_online(smp_processor_id()) &&
 	    system_state != SYSTEM_RUNNING) {
 		/*
@@ -554,24 +587,31 @@ asmlinkage int printk(const char *fmt, ...)
 		spin_unlock_irqrestore(&logbuf_lock, flags);
 		goto out;
 	}
-	if (!down_trylock(&console_sem)) {
-		console_locked = 1;
-		/*
-		 * We own the drivers.  We can drop the spinlock and let
-		 * release_console_sem() print the text
-		 */
-		spin_unlock_irqrestore(&logbuf_lock, flags);
-		console_may_schedule = 0;
-		release_console_sem();
-	} else {
-		/*
-		 * Someone else owns the drivers.  We drop the spinlock, which
-		 * allows the semaphore holder to proceed and to call the
-		 * console drivers with the output which we just produced.
-		 */
-		spin_unlock_irqrestore(&logbuf_lock, flags);
-	}
-out:
+#ifdef CONFIG_ADEOS_CORE
+ 	if (adp_current != adp_root || !down_trylock(&console_sem)) {
+#else /* !CONFIG_ADEOS_CORE */
+		if (!down_trylock(&console_sem)) {
+#endif
+			console_locked = 1;
+			/*
+			 * We own the drivers.  We can drop the spinlock and let
+			 * release_console_sem() print the text
+			 */
+			spin_unlock_irqrestore(&logbuf_lock, flags);
+			console_may_schedule = 0;
+			release_console_sem();
+		} else {
+			/*
+			 * Someone else owns the drivers.  We drop the spinlock, which
+			 * allows the semaphore holder to proceed and to call the
+			 * console drivers with the output which we just produced.
+			 */
+			spin_unlock_irqrestore(&logbuf_lock, flags);
+		}
+	out:
+#ifdef CONFIG_ADEOS_CORE
+		__adeos_restore_root_nosync(swflag);
+#endif
 	return printed_len;
 }
 EXPORT_SYMBOL(printk);
@@ -633,13 +673,51 @@ void release_console_sem(void)
 	}
 	console_locked = 0;
 	console_may_schedule = 0;
+
+#ifdef CONFIG_ADEOS_CORE
+	if (adp_root != adp_current) {
+	    spin_unlock_irqrestore(&logbuf_lock, flags);
+	    return;
+	}
+	spin_unlock_irqrestore(&logbuf_lock, flags);
+
+	up(&console_sem);
+
+#else /* !CONFIG_ADEOS_CORE */
 	up(&console_sem);
 	spin_unlock_irqrestore(&logbuf_lock, flags);
+#endif /* CONFIG_ADEOS_CORE */
 	if (wake_klogd && !oops_in_progress && waitqueue_active(&log_wait))
 		wake_up_interruptible(&log_wait);
 }
 EXPORT_SYMBOL(release_console_sem);

+#ifdef CONFIG_ADEOS_CORE
+void __adeos_sync_console (unsigned virq) {
+	/* This handler always runs on behalf of the root (Linux) domain. */
+
+	unsigned long flags;
+
+	spin_lock_irqsave(&logbuf_lock, flags);
+
+	/*
+	 * Not absolutely atomic wrt to the triggering point, but this is
+	 * harmless. We only try to reduce the useless triggers by a cheap
+	 * trick here.
+	 */
+
+	clear_bit(ADEOS_PPRINTK_FLAG,&adp_root->flags);
+
+	if (cpu_online(smp_processor_id()) && system_state == SYSTEM_RUNNING && !down_trylock(&console_sem)) {
+		console_locked = 1;
+		spin_unlock_irqrestore(&logbuf_lock, flags);
+		console_may_schedule = 0;
+		release_console_sem();
+	} else
+		spin_unlock_irqrestore(&logbuf_lock, flags);
+}
+#endif /* CONFIG_ADEOS_CORE */
+
 /** console_conditional_schedule - yield the CPU if required
  *
  * If the console code is currently allowed to sleep, and
diff --git a/kernel/sched.c b/kernel/sched.c
index 10c2581..56b782d 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -248,8 +248,21 @@ static DEFINE_PER_CPU(struct runqueue, runqueues);
  * Default context-switch locking:
  */
 #ifndef prepare_arch_switch
+
+#ifdef CONFIG_ADEOS_CORE
+#define prepare_arch_switch(rq,prev,next) \
+do { \
+    struct { struct task_struct *prev, *next; } arg = { (prev), (next) }; \
+    __adeos_schedule_head(&arg); \
+    adeos_hw_cli(); \
+} while(0)
+#else /* !CONFIG_ADEOS_CORE */
+
 # define prepare_arch_switch(rq, next)	do { } while (0)
-# define finish_arch_switch(rq, next)	spin_unlock_irq(&(rq)->lock)
+//# define finish_arch_switch(rq, next)	spin_unlock_irq(&(rq)->lock)
+//check for the previous line the karo patch says it should be commented
+#endif /* CONFIG_ADEOS_CORE*/
+
 # define task_running(rq, p)		((rq)->curr == (p))
 #endif

@@ -1043,6 +1056,12 @@ asmlinkage void schedule_tail(task_t *prev)

 	if (current->set_child_tid)
 		put_user(current->pid, current->set_child_tid);
+
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_enter_process();
+#endif /* CONFIG_ADEOS_CORE */
+
+
 }

 /*
@@ -2191,6 +2210,11 @@ asmlinkage void __sched schedule(void)
 	unsigned long long now;
 	unsigned long run_time;
 	int cpu, idx;
+
+#ifdef CONFIG_ADEOS_CORE
+	if (adp_current != adp_root) /* Let's be helpful and	conservative. */
+	    return;
+#endif /* CONFIG_ADEOS_CORE */

 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
@@ -2302,11 +2326,25 @@ switch_tasks:
 		rq->nr_switches++;
 		rq->curr = next;
 		++*switch_count;
-
-		prepare_arch_switch(rq, next);
+
+#ifdef CONFIG_ADEOS_CORE
+		prepare_arch_switch(rq, prev, next);
+#else /* !CONFIG_ADEOS_CORE */
+  		prepare_arch_switch(rq, next);
+#endif /* CONFIG_ADEOS_CORE */
 		prev = context_switch(rq, prev, next);
 		barrier();

+#ifdef CONFIG_ADEOS_CORE
+		if (likely(adp_pipelined))
+		    adeos_hw_sti();
+
+		if (__adeos_schedule_tail(prev) > 0)
+		    /* Some event handler asked for a truncated
+		       scheduling tail. Just obey. */
+		    return;
+#endif /* CONFIG_ADEOS_CORE */
+
 		finish_task_switch(prev);
 	} else
 		spin_unlock_irq(&rq->lock);
@@ -2319,6 +2357,20 @@ switch_tasks:

 EXPORT_SYMBOL(schedule);

+#ifdef CONFIG_ADEOS_CORE
+
+void __adeos_schedule_back_root (struct task_struct *prev)
+
+{
+    finish_task_switch(prev);
+    reacquire_kernel_lock(current);
+    preempt_enable_no_resched();
+}
+
+EXPORT_SYMBOL(__adeos_schedule_back_root);
+
+#endif /* CONFIG_ADEOS_CORE */
+
 #ifdef CONFIG_PREEMPT
 /*
  * this is is the entry point to schedule() from in-kernel preemption
@@ -2329,6 +2381,15 @@ asmlinkage void __sched preempt_schedule(void)
 {
 	struct thread_info *ti = current_thread_info();

+#ifdef CONFIG_ADEOS_CORE
+ 	/* The in-kernel preemption routine might be indirectly called
+ 	   from code running in other domains, so we must ensure that
+ 	   scheduling only takes place on behalf of the root (Linux)
+ 	   one. */
+	if (adp_current != adp_root)
+	    return;
+#endif /* CONFIG_ADEOS_CORE */
+
 	/*
 	 * If there is a non-zero preempt_count or interrupts are disabled,
 	 * we do not want to preempt the current task.  Just return..
@@ -2773,6 +2834,17 @@ static int setscheduler(pid_t pid, int policy, struct sched_param __user *param)
 	retval = security_task_setscheduler(p, policy, &lp);
 	if (retval)
 		goto out_unlock;
+
+#ifdef CONFIG_ADEOS_CORE
+	{
+	struct { struct task_struct *task; int policy; struct sched_param *param; } evdata = { p, policy, &lp };
+	if (__adeos_renice_process(&evdata))
+	    {
+	    retval = 0;
+	    goto out_unlock;
+	    }
+	}
+#endif /* CONFIG_ADEOS_CORE */

 	array = p->array;
 	if (array)
@@ -3897,7 +3969,7 @@ void __init sched_init(void)
 {
 	runqueue_t *rq;
 	int i, j, k;
-
+
 #ifdef CONFIG_SMP
 	/* Set up an initial dummy domain for early boot */
 	static struct sched_domain sched_domain_init;
@@ -3915,7 +3987,7 @@ void __init sched_init(void)
 	sched_group_init.next = &sched_group_init;
 	sched_group_init.cpu_power = SCHED_LOAD_SCALE;
 #endif
-
+
 	for (i = 0; i < NR_CPUS; i++) {
 		prio_array_t *array;

diff --git a/kernel/signal.c b/kernel/signal.c
index e734221..63c4cfa 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -557,6 +557,13 @@ void signal_wake_up(struct task_struct *t, int resume)

 	set_tsk_thread_flag(t, TIF_SIGPENDING);

+#ifdef CONFIG_ADEOS_CORE
+	{
+	struct { struct task_struct *t; } evdata = { t };
+	__adeos_kick_process(&evdata);
+	}
+#endif  /*CONFIG_ADEOS_CORE */
+
 	/*
 	 * If resume is set, we want to wake it up in the TASK_STOPPED case.
 	 * We don't check for TASK_STOPPED because there is a race with it
@@ -782,6 +789,17 @@ specific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)
 		BUG();
 #endif

+#ifdef CONFIG_ADEOS_CORE
+	/* If some domain handler in the pipeline doesn't ask for
+	   propagation, return success pretending that 'sig' was
+	   delivered. */
+	{
+	struct { struct task_struct *task; int sig; } evdata = { t, sig };
+	if (__adeos_signal_process(&evdata))
+	    goto out;
+	}
+#endif /* CONFIG_ADEOS_CORE */
+
 	if (((unsigned long)info > 2) && (info->si_code == SI_TIMER))
 		/*
 		 * Set up a return to indicate that we dropped the signal.
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index ae99088..f1a2e67 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -923,6 +923,10 @@ void __init sysctl_init(void)
 #ifdef CONFIG_PROC_FS
 	register_proc_table(root_table, proc_sys_root);
 	init_irq_proc();
+#ifdef CONFIG_ADEOS_CORE
+	__adeos_init_proc();
+#endif /* CONFIG_ADEOS_CORE */
+
 #endif
 }

diff --git a/kernel/timer.c b/kernel/timer.c
index 3a8162b..027a8c6 100644
--- a/kernel/timer.c
+++ b/kernel/timer.c
@@ -1502,4 +1502,4 @@ void msleep(unsigned int msecs)
 }

 EXPORT_SYMBOL(msleep);
-
+EXPORT_SYMBOL(do_timer);
diff --git a/scripts/kconfig/mconf.c b/scripts/kconfig/mconf.c
index b3c24cb..6973316 100644
--- a/scripts/kconfig/mconf.c
+++ b/scripts/kconfig/mconf.c
@@ -88,7 +88,7 @@ static char *args[1024], **argptr = args;
 static int indent;
 static struct termios ios_org;
 static int rows, cols;
-static struct menu *current_menu;
+struct menu *current_menu;
 static int child_count;
 static int do_resize;
 static int single_menu_mode;
